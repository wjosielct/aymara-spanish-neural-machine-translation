{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7374ee3",
   "metadata": {},
   "source": [
    "# **Entrenamiento Español $\\rightarrow$ Aymara**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7cb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentencepiece\n",
    "!pip install -q sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ad4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.amp import autocast, GradScaler\n",
    "import sentencepiece as spm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "import os\n",
    "import sacrebleu\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4766668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. DATASET Y DATALOADER\n",
    "# ==========================================\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_path, trgt_path):\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.src_lines = [line.strip() for line in f]\n",
    "        with open(trgt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.trgt_lines = [line.strip() for line in f]\n",
    "        assert len(self.src_lines) == len(self.trgt_lines)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = list(map(int, self.src_lines[idx].split()))\n",
    "        trgt = list(map(int, self.trgt_lines[idx].split()))\n",
    "        # Aseguramos long para embeddings\n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(trgt, dtype=torch.long)\n",
    "\n",
    "train_dataset = TranslationDataset(src_path=\"data/splits/ids/train.spanish\", trgt_path=\"data/splits/ids/train.aymara\")\n",
    "valid_dataset = TranslationDataset(src_path=\"data/splits/ids/valid.spanish\", trgt_path=\"data/splits/ids/valid.aymara\")\n",
    "test_dataset  = TranslationDataset(src_path=\"data/splits/ids/test.spanish\",  trgt_path=\"data/splits/ids/test.aymara\")\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tokenizer/SentencePiece.model\")\n",
    "PAD_ID, BOS_ID, EOS_ID = sp.pad_id(), sp.bos_id(), sp.eos_id()\n",
    "vocab_size = sp.vocab_size()\n",
    "\n",
    "def collate_fn_batch(batch):\n",
    "    src_batch, trgt_batch = zip(*batch)\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=PAD_ID)\n",
    "    trgt_padded = pad_sequence(trgt_batch, batch_first=True, padding_value=PAD_ID)\n",
    "    return src_padded, trgt_padded\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_batch)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_batch)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5305004a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. TRANSFORMER\n",
    "# ==========================================\n",
    "\n",
    "class TranslationTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, n_heads=8, num_layers=6, dim_ffnn=2048, dropout=0.2, pad_id=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model # Guardamos d_model para el escalado\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.positional_encoder = nn.Embedding(150, d_model) # 150 es seguro para max_len 80\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_ffnn,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=d_model, out_features=vocab_size, bias=False)\n",
    "        self.output_layer.weight = self.embedding.weight \n",
    "\n",
    "    def forward(self, src, trgt, trgt_causal_mask=None, src_key_padding_mask=None, trgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        src_pos = self.positional_encoder(torch.arange(src.size(1), device=src.device))\n",
    "        trgt_pos = self.positional_encoder(torch.arange(trgt.size(1), device=trgt.device))\n",
    "\n",
    "        # Escalado por sqrt(d_model)\n",
    "        src = (self.embedding(src) * math.sqrt(self.d_model)) + src_pos\n",
    "        trgt = (self.embedding(trgt) * math.sqrt(self.d_model)) + trgt_pos\n",
    "\n",
    "        out = self.transformer(\n",
    "            src,\n",
    "            trgt,\n",
    "            tgt_mask=trgt_causal_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=trgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        return self.output_layer(out)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07eef9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. INICIALIZACIÓN\n",
    "# ==========================================\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "model = TranslationTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    num_layers=6,\n",
    "    dim_ffnn=2048,\n",
    "    dropout=0.1,\n",
    "    pad_id=PAD_ID\n",
    ").to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.0005, total_steps=total_steps, pct_start=0.1, anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "700d053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. FUNCIONES DE ENTRENAMIENTO Y DECODING\n",
    "# ==========================================\n",
    "\n",
    "def create_padding_mask(batch, pad_id):\n",
    "    return (batch == pad_id)\n",
    "\n",
    "def create_causal_mask(size, device):\n",
    "    return nn.Transformer.generate_square_subsequent_mask(size).to(device)\n",
    "\n",
    "def train_epoch_amp(model, data_loader, optimizer, criterion, device, pad_id, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for src, trgt in data_loader:\n",
    "        src, trgt = src.to(device), trgt.to(device)\n",
    "        trgt_in, trgt_out = trgt[:, :-1], trgt[:, 1:].contiguous()\n",
    "\n",
    "        trgt_causal_mask = create_causal_mask(trgt_in.size(1), device)\n",
    "        src_mask = create_padding_mask(src, pad_id)\n",
    "        trgt_mask = create_padding_mask(trgt_in, pad_id)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            logits = model(src, trgt_in, trgt_causal_mask, src_mask, trgt_mask, src_mask)\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), trgt_out.reshape(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, data_loader, criterion, device, pad_id):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for src, trgt in data_loader:\n",
    "        src, trgt = src.to(device), trgt.to(device)\n",
    "        trgt_in, trgt_out = trgt[:, :-1], trgt[:, 1:].contiguous()\n",
    "        \n",
    "        trgt_causal_mask = create_causal_mask(trgt_in.size(1), device)\n",
    "        src_mask, trgt_mask = create_padding_mask(src, pad_id), create_padding_mask(trgt_in, pad_id)\n",
    "\n",
    "        logits = model(src, trgt_in, trgt_causal_mask, src_mask, trgt_mask, src_mask)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), trgt_out.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_search_decode(model, src, sp, beam_size, max_len, device):\n",
    "    PAD_ID, BOS_ID, EOS_ID = sp.pad_id(), sp.bos_id(), sp.eos_id()\n",
    "    src = src.unsqueeze(0).to(device)\n",
    "    src_mask = create_padding_mask(src, PAD_ID).to(device)\n",
    "    \n",
    "    # Encoding manual debe considerar el escalado sqrt(d_model)\n",
    "    src_emb = model.embedding(src) * math.sqrt(model.d_model)\n",
    "    src_pos = model.positional_encoder(torch.arange(src.size(1), device=device))\n",
    "    memory = model.transformer.encoder(src_emb + src_pos, src_key_padding_mask=src_mask)\n",
    "\n",
    "    candidates = [([BOS_ID], 0.0)]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        new_candidates = []\n",
    "        all_finished = True\n",
    "        \n",
    "        for seq, score in candidates:\n",
    "            if seq[-1] == EOS_ID:\n",
    "                new_candidates.append((seq, score))\n",
    "                continue\n",
    "            \n",
    "            all_finished = False\n",
    "            trgt_in = torch.tensor([seq], device=device)\n",
    "            \n",
    "            # Decoder forward manual también con escalado\n",
    "            trgt_emb = model.embedding(trgt_in) * math.sqrt(model.d_model)\n",
    "            trgt_pos = model.positional_encoder(torch.arange(trgt_in.size(1), device=device))\n",
    "            \n",
    "            trgt_causal_mask = create_causal_mask(trgt_in.size(1), device)\n",
    "            out = model.transformer.decoder(trgt_emb + trgt_pos, memory, tgt_mask=trgt_causal_mask, memory_key_padding_mask=src_mask)\n",
    "            \n",
    "            log_probs = torch.log_softmax(model.output_layer(out[:, -1, :]), dim=-1)\n",
    "            topk_probs, topk_ids = torch.topk(log_probs, beam_size, dim=-1)\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                new_candidates.append((seq + [topk_ids[0][i].item()], score + topk_probs[0][i].item()))\n",
    "        \n",
    "        if all_finished: break\n",
    "        candidates = sorted(new_candidates, key=lambda x: x[1] / (len(x[0]) ** 0.7), reverse=True)[:beam_size]\n",
    "        \n",
    "    return candidates[0][0]\n",
    "\n",
    "def bleu_epoch(model, dataset, sp, device, beam_size=3, max_len=80, limit=1000):\n",
    "    model.eval()\n",
    "    hypotheses, references = [], []\n",
    "    \n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    if limit is None:\n",
    "        n_eval = total_samples # Si es None, usamos todo el data set\n",
    "    else:\n",
    "        n_eval = min(total_samples, limit)\n",
    "        \n",
    "    indices = range(n_eval)\n",
    "    # ------------------\n",
    "    \n",
    "    if n_eval > 1000:\n",
    "        print(f\"Evaluando {n_eval} oraciones (esto tomará un tiempo)...\")\n",
    "\n",
    "    for idx in indices:\n",
    "        src, trgt = dataset[idx]\n",
    "        references.append(sp.decode(trgt.tolist()))\n",
    "        hypotheses.append(sp.decode(beam_search_decode(model, src, sp, beam_size, max_len, device)))\n",
    "        \n",
    "    return sacrebleu.corpus_bleu(hypotheses, [references]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3753cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando 100 épocas. Validando BLEU cada 10 épocas (max 1000 oraciones).\n",
      "Epoch 1 | LR: 0.00003175 | Train Loss: 7.4147 | Val Loss: 6.6554 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 2 | LR: 0.00006584 | Train Loss: 6.2854 | Val Loss: 5.9080 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 3 | LR: 0.00011895 | Train Loss: 5.6100 | Val Loss: 5.2433 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 4 | LR: 0.00018586 | Train Loss: 4.9462 | Val Loss: 4.5795 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 5 | LR: 0.00026003 | Train Loss: 4.3301 | Val Loss: 4.0488 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 6 | LR: 0.00033420 | Train Loss: 3.8874 | Val Loss: 3.7171 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 7 | LR: 0.00040111 | Train Loss: 3.5775 | Val Loss: 3.4902 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 8 | LR: 0.00045419 | Train Loss: 3.3540 | Val Loss: 3.3473 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 9 | LR: 0.00048827 | Train Loss: 3.1786 | Val Loss: 3.2154 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "--> ¡Nuevo récord BLEU: 8.19!\n",
      "Epoch 10 | LR: 0.00050000 | Train Loss: 3.0290 | Val Loss: 3.1125 | Val BLEU: 8.19\n",
      "------------------------------------------------------------\n",
      "Epoch 11 | LR: 0.00049985 | Train Loss: 2.8896 | Val Loss: 3.0013 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 12 | LR: 0.00049939 | Train Loss: 2.7673 | Val Loss: 2.9150 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 13 | LR: 0.00049863 | Train Loss: 2.6670 | Val Loss: 2.8397 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 14 | LR: 0.00049757 | Train Loss: 2.5831 | Val Loss: 2.7899 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 15 | LR: 0.00049620 | Train Loss: 2.5130 | Val Loss: 2.7282 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 16 | LR: 0.00049454 | Train Loss: 2.4529 | Val Loss: 2.6813 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 17 | LR: 0.00049257 | Train Loss: 2.3996 | Val Loss: 2.6378 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 18 | LR: 0.00049031 | Train Loss: 2.3526 | Val Loss: 2.6087 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 19 | LR: 0.00048776 | Train Loss: 2.3109 | Val Loss: 2.5663 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "--> ¡Nuevo récord BLEU: 12.50!\n",
      "Epoch 20 | LR: 0.00048492 | Train Loss: 2.2728 | Val Loss: 2.5300 | Val BLEU: 12.50\n",
      "------------------------------------------------------------\n",
      "Epoch 21 | LR: 0.00048179 | Train Loss: 2.2380 | Val Loss: 2.5083 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 22 | LR: 0.00047838 | Train Loss: 2.2067 | Val Loss: 2.4864 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 23 | LR: 0.00047470 | Train Loss: 2.1770 | Val Loss: 2.4538 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 24 | LR: 0.00047073 | Train Loss: 2.1488 | Val Loss: 2.4372 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 25 | LR: 0.00046650 | Train Loss: 2.1238 | Val Loss: 2.4116 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 26 | LR: 0.00046201 | Train Loss: 2.0984 | Val Loss: 2.3894 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 27 | LR: 0.00045726 | Train Loss: 2.0772 | Val Loss: 2.3738 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 28 | LR: 0.00045225 | Train Loss: 2.0541 | Val Loss: 2.3544 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 29 | LR: 0.00044700 | Train Loss: 2.0322 | Val Loss: 2.3322 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "--> ¡Nuevo récord BLEU: 14.55!\n",
      "Epoch 30 | LR: 0.00044151 | Train Loss: 2.0127 | Val Loss: 2.3059 | Val BLEU: 14.55\n",
      "------------------------------------------------------------\n",
      "Epoch 31 | LR: 0.00043578 | Train Loss: 1.9928 | Val Loss: 2.2983 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 32 | LR: 0.00042983 | Train Loss: 1.9744 | Val Loss: 2.2760 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 33 | LR: 0.00042366 | Train Loss: 1.9565 | Val Loss: 2.2645 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 34 | LR: 0.00041728 | Train Loss: 1.9388 | Val Loss: 2.2471 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 35 | LR: 0.00041069 | Train Loss: 1.9210 | Val Loss: 2.2366 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 36 | LR: 0.00040391 | Train Loss: 1.9042 | Val Loss: 2.2197 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 37 | LR: 0.00039694 | Train Loss: 1.8881 | Val Loss: 2.1997 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 38 | LR: 0.00038979 | Train Loss: 1.8735 | Val Loss: 2.1911 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 39 | LR: 0.00038247 | Train Loss: 1.8576 | Val Loss: 2.1763 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 40 | LR: 0.00037499 | Train Loss: 1.8440 | Val Loss: 2.1644 | Val BLEU: 14.19\n",
      "------------------------------------------------------------\n",
      "Epoch 41 | LR: 0.00036736 | Train Loss: 1.8312 | Val Loss: 2.1559 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 42 | LR: 0.00035959 | Train Loss: 1.8145 | Val Loss: 2.1427 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 43 | LR: 0.00035168 | Train Loss: 1.8003 | Val Loss: 2.1294 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 44 | LR: 0.00034365 | Train Loss: 1.7866 | Val Loss: 2.1123 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 45 | LR: 0.00033550 | Train Loss: 1.7729 | Val Loss: 2.1015 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 46 | LR: 0.00032725 | Train Loss: 1.7600 | Val Loss: 2.0875 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 47 | LR: 0.00031890 | Train Loss: 1.7476 | Val Loss: 2.0741 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 48 | LR: 0.00031047 | Train Loss: 1.7357 | Val Loss: 2.0661 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 49 | LR: 0.00030197 | Train Loss: 1.7238 | Val Loss: 2.0572 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 50 | LR: 0.00029341 | Train Loss: 1.7120 | Val Loss: 2.0479 | Val BLEU: 13.62\n",
      "------------------------------------------------------------\n",
      "Epoch 51 | LR: 0.00028479 | Train Loss: 1.7005 | Val Loss: 2.0370 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 52 | LR: 0.00027613 | Train Loss: 1.6891 | Val Loss: 2.0264 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 53 | LR: 0.00026743 | Train Loss: 1.6784 | Val Loss: 2.0175 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 54 | LR: 0.00025872 | Train Loss: 1.6681 | Val Loss: 2.0061 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 55 | LR: 0.00024999 | Train Loss: 1.6578 | Val Loss: 1.9971 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 56 | LR: 0.00024127 | Train Loss: 1.6477 | Val Loss: 1.9946 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 57 | LR: 0.00023255 | Train Loss: 1.6380 | Val Loss: 1.9826 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 58 | LR: 0.00022386 | Train Loss: 1.6284 | Val Loss: 1.9725 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 59 | LR: 0.00021520 | Train Loss: 1.6194 | Val Loss: 1.9649 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 60 | LR: 0.00020658 | Train Loss: 1.6101 | Val Loss: 1.9642 | Val BLEU: 13.61\n",
      "------------------------------------------------------------\n",
      "Epoch 61 | LR: 0.00019802 | Train Loss: 1.6015 | Val Loss: 1.9568 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 62 | LR: 0.00018951 | Train Loss: 1.5936 | Val Loss: 1.9481 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 63 | LR: 0.00018108 | Train Loss: 1.5856 | Val Loss: 1.9433 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 64 | LR: 0.00017274 | Train Loss: 1.5778 | Val Loss: 1.9380 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 65 | LR: 0.00016449 | Train Loss: 1.5705 | Val Loss: 1.9305 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 66 | LR: 0.00015634 | Train Loss: 1.5633 | Val Loss: 1.9244 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 67 | LR: 0.00014831 | Train Loss: 1.5565 | Val Loss: 1.9166 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 68 | LR: 0.00014040 | Train Loss: 1.5495 | Val Loss: 1.9094 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 69 | LR: 0.00013263 | Train Loss: 1.5432 | Val Loss: 1.9055 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 70 | LR: 0.00012499 | Train Loss: 1.5368 | Val Loss: 1.9023 | Val BLEU: 12.63\n",
      "------------------------------------------------------------\n",
      "Epoch 71 | LR: 0.00011752 | Train Loss: 1.5309 | Val Loss: 1.8990 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 72 | LR: 0.00011020 | Train Loss: 1.5250 | Val Loss: 1.8917 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 73 | LR: 0.00010305 | Train Loss: 1.5200 | Val Loss: 1.8906 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 74 | LR: 0.00009608 | Train Loss: 1.5146 | Val Loss: 1.8880 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 75 | LR: 0.00008930 | Train Loss: 1.5095 | Val Loss: 1.8836 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 76 | LR: 0.00008271 | Train Loss: 1.5048 | Val Loss: 1.8810 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 77 | LR: 0.00007633 | Train Loss: 1.5008 | Val Loss: 1.8747 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 78 | LR: 0.00007016 | Train Loss: 1.4966 | Val Loss: 1.8747 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 79 | LR: 0.00006421 | Train Loss: 1.4924 | Val Loss: 1.8715 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 80 | LR: 0.00005849 | Train Loss: 1.4888 | Val Loss: 1.8695 | Val BLEU: 12.03\n",
      "------------------------------------------------------------\n",
      "Epoch 81 | LR: 0.00005299 | Train Loss: 1.4854 | Val Loss: 1.8678 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 82 | LR: 0.00004774 | Train Loss: 1.4821 | Val Loss: 1.8644 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 83 | LR: 0.00004274 | Train Loss: 1.4790 | Val Loss: 1.8652 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 84 | LR: 0.00003799 | Train Loss: 1.4761 | Val Loss: 1.8626 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 85 | LR: 0.00003349 | Train Loss: 1.4736 | Val Loss: 1.8614 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 86 | LR: 0.00002926 | Train Loss: 1.4710 | Val Loss: 1.8596 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 87 | LR: 0.00002530 | Train Loss: 1.4689 | Val Loss: 1.8590 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 88 | LR: 0.00002161 | Train Loss: 1.4669 | Val Loss: 1.8567 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 89 | LR: 0.00001820 | Train Loss: 1.4651 | Val Loss: 1.8571 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 90 | LR: 0.00001508 | Train Loss: 1.4635 | Val Loss: 1.8565 | Val BLEU: 12.28\n",
      "------------------------------------------------------------\n",
      "Epoch 91 | LR: 0.00001224 | Train Loss: 1.4620 | Val Loss: 1.8557 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 92 | LR: 0.00000968 | Train Loss: 1.4608 | Val Loss: 1.8565 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 93 | LR: 0.00000743 | Train Loss: 1.4600 | Val Loss: 1.8548 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 94 | LR: 0.00000546 | Train Loss: 1.4591 | Val Loss: 1.8551 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 95 | LR: 0.00000380 | Train Loss: 1.4583 | Val Loss: 1.8544 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 96 | LR: 0.00000243 | Train Loss: 1.4577 | Val Loss: 1.8542 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 97 | LR: 0.00000137 | Train Loss: 1.4574 | Val Loss: 1.8547 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 98 | LR: 0.00000061 | Train Loss: 1.4571 | Val Loss: 1.8546 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 99 | LR: 0.00000015 | Train Loss: 1.4568 | Val Loss: 1.8544 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 100 | LR: 0.00000000 | Train Loss: 1.4570 | Val Loss: 1.8543 | Val BLEU: 12.33\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# LOOP PRINCIPAL\n",
    "# ==========================================\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_bleu = 0.0\n",
    "training_history = []\n",
    "\n",
    "print(f\"Entrenando {EPOCHS} épocas. Validando BLEU cada 10 épocas (max 1000 oraciones).\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_epoch_amp(model, train_loader, optimizer, criterion, device, PAD_ID, scheduler)\n",
    "    val_loss = eval_epoch(model, valid_loader, criterion, device, PAD_ID)\n",
    "    val_ppl = math.exp(min(val_loss, 100))\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"models/best_loss_model_SpanishToAymara_Standard.pt\")\n",
    "\n",
    "    bleu_score = None\n",
    "    if epoch % 10 == 0 or epoch == EPOCHS:\n",
    "        print(\"--> Calculando BLEU...\")\n",
    "        # Usamos beam_size=3 para calidad\n",
    "        bleu_score = bleu_epoch(model, valid_dataset, sp, device, beam_size=3, max_len=80, limit=1000)\n",
    "        \n",
    "        if bleu_score > best_bleu:\n",
    "            best_bleu = bleu_score\n",
    "            torch.save(model.state_dict(), \"models/best_bleu_model_SpanishToAymara_Standard.pt\")\n",
    "            print(f\"--> ¡Nuevo récord BLEU: {best_bleu:.2f}!\")\n",
    "\n",
    "    bleu_str = f\"{bleu_score:.2f}\" if bleu_score is not None else \"-\"\n",
    "    print(f\"Epoch {epoch} | LR: {current_lr:.8f} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val BLEU: {bleu_str}\")\n",
    "\n",
    "    training_history.append({\n",
    "        \"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss,\n",
    "        \"val_ppl\": val_ppl, \"val_bleu\": bleu_score, \"lr\": current_lr\n",
    "    })\n",
    "    pd.DataFrame(training_history).to_csv(\"results/metrics_SpanishToAymara_Standard.csv\", index=False)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d837a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el mejor modelo para evaluación del conjunto de prueba...\n",
      "--> Cargando el mejor modelo guardado: models/best_bleu_model_SpanishToAymara.pt\n",
      "Pesos del modelo cargados exitosamente!\n",
      "\n",
      "Calculando métricas en el Test Set completo...\n",
      "Evaluando 8187 oraciones (esto tomará un tiempo)...\n",
      "\n",
      "Resultados Finales del Test Set:\n",
      "===================================\n",
      "Test Loss: 2.3192\n",
      "Test PPL : 10.17\n",
      "Test BLEU: 13.37 (Beam=5)\n",
      "===================================\n",
      "\n",
      "--- Ejemplos de Traducción ---\n",
      "Español:  pero a ninguno de los hijos de israel sometió a servidumbre para sus obras; porque ellos eran hombres de guerra, jefes de sus comandantes, jefes de sus carros y sus jinetes.\n",
      "Aymara: ucampis janiw qhiti israelita jakerusa mä esclavjama luräwinacapanjja sirviyascänti, jan ucasti jupanacajj soldadonacäpjjänwa, jilïrinaca, capitananaca, uqhamarac comandantenacäpjjänwa nuwasiñ carronacampin, caballerianacampina.\n",
      "Modelo:  ukampis janiw khiti israelita jaqirusa uka jaqinakarjam irnaqaykänti, jan ukasti, jupanakarux yaqha irnaqäwitakiw uchäna, soldadotaki, yanapiritaki, nuwasïwir sarañ carronakana, caballerianakan jilïrinakapataki.\n",
      "--------------------------------------------------\n",
      "Español:  todos ellos se sentaron en la plaza del templo de dios, temblando por causa de aquel asunto y de la lluvia que caía.\n",
      "Aymara: jupanakasti taqiniw tatitun utapan utjkäna uka plazan khathatt'asis qunt'asipxäna, uka arunak yatisina, ukhamarak jallun purintatapata.\n",
      "Modelo:  jupanakasti taqpachaniw diosan utapan qunt'asipxäna, khathatt'asisa diosat jutatap layku.\n",
      "--------------------------------------------------\n",
      "Español:  oigan esta palabra, oh vacas de basán que están en el monte de samaria, que oprimen a los pobres, que quebrantan a los necesitados, que dicen a sus maridos: \"¡traigan y bebamos!\".\n",
      "Aymara: samariankir qamir warminaka, aka arunak ist'apxam, jumanakax wajchanakaruw jachayapxtaxa, jan kunaniruw t'aqhisiyapxtaxa, chachanakamarusti, machañatak vino mayipxtaxa.\n",
      "Modelo:  ist'apjjam aca arunaca, basananquir kamir warminaca, qhitinacatejj samaria kollunacan utjapjjta ucanaca, jumanacasti pobrenacarojj jachayapjjtawa, jan cunanirusti t'akhesiyapjjaractawa, chachanacamarusti: \"vino apanipjjam umañataqui\" sapjjaractawa.\n",
      "--------------------------------------------------\n",
      "Español:  cruza el paso del torrente, acampa en geba; ramá se llena de terror, gabaa de saúl sale huyendo.\n",
      "Aymara: c'ullc'u thaqui chekwa pasaniwayapjje, geba sisqui uca chekaruw utjantasipjjaraqui, ramá marcasti sustjasiwa, saulon gabaa sat marcapasti jaltjjewa.\n",
      "Modelo:  k'ullk'u thakhi chiq sarakipanipxi, gueba siski uka chiqaruw qurpachasipxi, ramá markankirinakasti mulljasipxiwa, saúl reyin guibeá markapan jakirinakasti jaltxapxarakiwa.\n",
      "--------------------------------------------------\n",
      "Español:  \"en tu corazón de piedra, judá, tu pecado está escrito con cincel de hierro; en las esquinas de tus altares está grabado con punta de diamante.\n",
      "Aymara: \"judá marca, juchamajj hierro cincelampi kellkatawa, diamante puntampi kellkatawa, kala chuymamjjaru, altaranacaman wajjranacaparusa.\n",
      "Modelo:  \"judá marka, juchamax hierro cincelampi qillqantatawa, qala chuymamxar diamante arimpi qillqantatawa, altaranakamas juchanakamampi qillqantatawa.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# EVALUACIÓN FINAL EN TEST SET\n",
    "# ==========================================\n",
    "\n",
    "print(\"Cargando el mejor modelo para evaluación del conjunto de prueba...\")\n",
    "\n",
    "# 1. Re-inicializamos el modelo limpio\n",
    "model_test = TranslationTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    num_layers=6,\n",
    "    dim_ffnn=2048,\n",
    "    dropout=0.0, # Dropout no se usa en inferencia\n",
    "    pad_id=PAD_ID\n",
    ").to(device)\n",
    "\n",
    "# 2. Cargas los pesos. \n",
    "model_path = \"models/best_bleu_model_SpanishToAymara_Standard.pt\"\n",
    "print(f\"--> Cargando el mejor modelo guardado: {model_path}\")\n",
    "model_test.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(\"Pesos del modelo cargados exitosamente!\")\n",
    "\n",
    "model_test.eval()\n",
    "\n",
    "# ------------------------------------------\n",
    "# A. Cálculo de Métricas Cuantitativas\n",
    "# ------------------------------------------\n",
    "print(\"\\nCalculando métricas en el Test Set completo...\")\n",
    "\n",
    "# 1. Loss y Perplexity\n",
    "test_loss = eval_epoch(model_test, test_loader, criterion, device, PAD_ID)\n",
    "test_ppl = math.exp(min(test_loss, 100))\n",
    "\n",
    "# 2. BLEU Score\n",
    "# Nota: Ponemos limit=None para evaluar el test set completo\n",
    "# Nota: Beam size=5 es estándar para publicaciones y resultados finales\n",
    "test_bleu = bleu_epoch(model_test, test_dataset, sp, device, beam_size=5, max_len=80, limit=None)\n",
    "\n",
    "print(f\"\\nResultados Finales del Test Set:\")\n",
    "print(f\"===================================\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test PPL : {test_ppl:.2f}\")\n",
    "print(f\"Test BLEU: {test_bleu:.2f} (Beam=5)\")\n",
    "print(f\"===================================\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# B. Evaluación Cualitativa (Ver traducciones)\n",
    "# ------------------------------------------\n",
    "print(\"\\n--- Ejemplos de Traducción ---\")\n",
    "indices = random.sample(range(len(test_dataset)), 5) # 5 ejemplos al azar\n",
    "\n",
    "for idx in indices:\n",
    "    src, trgt = test_dataset[idx]\n",
    "    \n",
    "    src_text = sp.decode(src.tolist())\n",
    "    trgt_text = sp.decode(trgt.tolist())\n",
    "    \n",
    "    # Inferencia\n",
    "    pred_ids = beam_search_decode(model_test, src, sp, beam_size=5, max_len=80, device=device)\n",
    "    pred_text = sp.decode(pred_ids)\n",
    "    \n",
    "    print(f\"Español:  {src_text}\")\n",
    "    print(f\"Aymara: {trgt_text}\")\n",
    "    print(f\"Modelo:  {pred_text}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc9723-71f8-4ca6-82f2-78c098a999c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
