{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab964476",
   "metadata": {},
   "source": [
    "## **Librerías a usar en la extracción de los pares del PDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57a791cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3419612d",
   "metadata": {},
   "source": [
    "## Funciones para extraer conversaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f9b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_conversaciones_a_txt(ruta_pdf):\n",
    "    # 1. Crear carpeta para guardar los archivos si no existe\n",
    "    carpeta_salida = \"data/raw/conversaciones_inicial\"\n",
    "    os.makedirs(carpeta_salida, exist_ok=True)\n",
    "\n",
    "    # 2. Configuración\n",
    "    # Páginas 13 a 146\n",
    "    pagina_inicio = 12 \n",
    "    pagina_fin = 146 \n",
    "    \n",
    "    # Patrón para detectar el INICIO de una nueva conversación\n",
    "    # Busca números seguidos de ° o º (ej: \"1° -\", \"2º -\", \"40° -\")\n",
    "    patron_inicio_capitulo = re.compile(r\"^\\d+\\s*[°º]\")\n",
    "    \n",
    "    # Buffer para ir guardando las líneas de la conversación actual\n",
    "    conversacion_actual = []\n",
    "    contador_conversaciones = 0\n",
    "\n",
    "    print(f\"Abriendo PDF: {ruta_pdf}...\")\n",
    "\n",
    "    with pdfplumber.open(ruta_pdf) as pdf:\n",
    "        # Iteramos solo sobre las páginas indicadas\n",
    "        # Usamos slice [pagina_inicio : pagina_fin] para ir directo al grano\n",
    "        paginas_interes = pdf.pages[pagina_inicio:pagina_fin]\n",
    "        \n",
    "        print(f\"Procesando {len(paginas_interes)} páginas...\")\n",
    "\n",
    "        for pagina in paginas_interes:\n",
    "            texto = pagina.extract_text()\n",
    "            if not texto:\n",
    "                continue\n",
    "            \n",
    "            lineas = texto.split('\\n')\n",
    "            \n",
    "            for linea in lineas:\n",
    "                linea = linea.strip()\n",
    "                \n",
    "                # Detectamos si la línea es el inicio de una NUEVA conversación\n",
    "                if patron_inicio_capitulo.match(linea):\n",
    "                    \n",
    "                    # Si ya teníamos una conversación acumulándose, la guardamos antes de empezar la nueva\n",
    "                    if conversacion_actual:\n",
    "                        guardar_archivo(carpeta_salida, contador_conversaciones, conversacion_actual)\n",
    "                    \n",
    "                    # Reiniciamos para la nueva conversación\n",
    "                    conversacion_actual = []\n",
    "                    contador_conversaciones += 1\n",
    "                    conversacion_actual.append(linea) # Añadimos el título como primera línea\n",
    "                \n",
    "                else:\n",
    "                    # Si no es título nuevo, es contenido de la conversación actual\n",
    "                    # (Solo guardamos si ya hemos encontrado al menos el primer título)\n",
    "                    if contador_conversaciones > 0:\n",
    "                        conversacion_actual.append(linea)\n",
    "\n",
    "        # 3. Guardar la última conversación que quedó en memoria al terminar el loop\n",
    "        if conversacion_actual:\n",
    "             guardar_archivo(carpeta_salida, contador_conversaciones, conversacion_actual)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"¡Proceso completado! Se crearon {contador_conversaciones} archivos en la carpeta '{carpeta_salida}'.\")\n",
    "\n",
    "def guardar_archivo(carpeta, numero, lineas):\n",
    "    \"\"\"Función auxiliar para escribir el archivo .txt\"\"\"\n",
    "    nombre_archivo = f\"conversacion_{numero}.txt\"\n",
    "    ruta_completa = os.path.join(carpeta, nombre_archivo)\n",
    "    \n",
    "    contenido = \"\\n\".join(lineas)\n",
    "    \n",
    "    with open(ruta_completa, 'w', encoding='utf-8') as f:\n",
    "        f.write(contenido)\n",
    "    print(f\"Guardado: {nombre_archivo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2dbd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abriendo PDF: data/raw/book/book.pdf...\n",
      "Procesando 134 páginas...\n",
      "Guardado: conversacion_1.txt\n",
      "Guardado: conversacion_2.txt\n",
      "Guardado: conversacion_3.txt\n",
      "Guardado: conversacion_4.txt\n",
      "Guardado: conversacion_5.txt\n",
      "Guardado: conversacion_6.txt\n",
      "Guardado: conversacion_7.txt\n",
      "Guardado: conversacion_8.txt\n",
      "Guardado: conversacion_9.txt\n",
      "Guardado: conversacion_10.txt\n",
      "Guardado: conversacion_11.txt\n",
      "Guardado: conversacion_12.txt\n",
      "Guardado: conversacion_13.txt\n",
      "Guardado: conversacion_14.txt\n",
      "Guardado: conversacion_15.txt\n",
      "Guardado: conversacion_16.txt\n",
      "Guardado: conversacion_17.txt\n",
      "Guardado: conversacion_18.txt\n",
      "Guardado: conversacion_19.txt\n",
      "Guardado: conversacion_20.txt\n",
      "Guardado: conversacion_21.txt\n",
      "Guardado: conversacion_22.txt\n",
      "Guardado: conversacion_23.txt\n",
      "Guardado: conversacion_24.txt\n",
      "Guardado: conversacion_25.txt\n",
      "Guardado: conversacion_26.txt\n",
      "Guardado: conversacion_27.txt\n",
      "Guardado: conversacion_28.txt\n",
      "Guardado: conversacion_29.txt\n",
      "Guardado: conversacion_30.txt\n",
      "Guardado: conversacion_31.txt\n",
      "Guardado: conversacion_32.txt\n",
      "Guardado: conversacion_33.txt\n",
      "Guardado: conversacion_34.txt\n",
      "Guardado: conversacion_35.txt\n",
      "Guardado: conversacion_36.txt\n",
      "Guardado: conversacion_37.txt\n",
      "Guardado: conversacion_38.txt\n",
      "Guardado: conversacion_39.txt\n",
      "Guardado: conversacion_40.txt\n",
      "------------------------------\n",
      "¡Proceso completado! Se crearon 40 archivos en la carpeta 'data/raw/conversaciones_inicial'.\n"
     ]
    }
   ],
   "source": [
    "# Extraemos conversaciones\n",
    "archivo_pdf = 'data/raw/book/book.pdf'\n",
    "extraer_conversaciones_a_txt(archivo_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa27123",
   "metadata": {},
   "source": [
    "### Función para extraer la parte hablada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b1efee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiar_conversaciones():\n",
    "    carp_origen = \"data/raw/conversaciones_inicial\"\n",
    "    carp_destino = \"data/raw/conversaciones_final\"\n",
    "    \n",
    "    # Textos a eliminar\n",
    "    frases_prohibidas = [\n",
    "        \"conversaciones en aimara\",\n",
    "        \"aymara aruskipawinaka\"\n",
    "    ]\n",
    "    separador_clave = \"-------------------&-------------------\"\n",
    "\n",
    "    # Crear carpeta de destino si no existe\n",
    "    os.makedirs(carp_destino, exist_ok=True)\n",
    "        \n",
    "    print(f\"Limpiando archivos de '{carp_origen}' a '{carp_destino}'...\")\n",
    "\n",
    "    # Iterar sobre cada archivo en la carpeta origen\n",
    "    archivos = sorted([f for f in os.listdir(carp_origen) if f.endswith(\".txt\")])\n",
    "    \n",
    "    for nombre_archivo in archivos:\n",
    "        ruta_origen = os.path.join(carp_origen, nombre_archivo)\n",
    "        ruta_destino = os.path.join(carp_destino, nombre_archivo)\n",
    "        \n",
    "        with open(ruta_origen, 'r', encoding='utf-8') as f:\n",
    "            lineas = f.readlines()\n",
    "            \n",
    "        lineas_limpias = []\n",
    "        encontrado_separador = False\n",
    "        \n",
    "        for linea in lineas:\n",
    "            linea_stripped = linea.strip()\n",
    "            \n",
    "            # 1. Lógica del Separador\n",
    "            # Si aún no encontramos el separador, revisamos si esta línea lo es\n",
    "            if not encontrado_separador:\n",
    "                if separador_clave in linea:\n",
    "                    encontrado_separador = True\n",
    "                continue # Saltamos todo lo que esté antes (y la línea del separador también)\n",
    "            \n",
    "            # --- A partir de aquí estamos DESPUÉS del separador ---\n",
    "            \n",
    "            # 2. Eliminar líneas vacías\n",
    "            if not linea_stripped:\n",
    "                continue\n",
    "                \n",
    "            # 3. Eliminar líneas que solo tienen números (números de página)\n",
    "            if linea_stripped.isdigit():\n",
    "                continue\n",
    "                \n",
    "            # 4. Eliminar líneas con frases prohibidas (Encabezados)\n",
    "            if any(frase in linea.lower() for frase in frases_prohibidas):\n",
    "                continue\n",
    "            \n",
    "            # Si pasa todos los filtros, la guardamos\n",
    "            lineas_limpias.append(linea)\n",
    "            \n",
    "        # Guardar el archivo limpio\n",
    "        if lineas_limpias:\n",
    "            with open(ruta_destino, 'w', encoding='utf-8') as f_out:\n",
    "                f_out.writelines(lineas_limpias)\n",
    "                \n",
    "    print(f\"¡Listo! Se han procesado {len(archivos)} conversaciones.\")\n",
    "    print(f\"Revisa la carpeta: {carp_destino}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "187867c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpiando archivos de 'data/raw/conversaciones_inicial' a 'data/raw/conversaciones_final'...\n",
      "¡Listo! Se han procesado 40 conversaciones.\n",
      "Revisa la carpeta: data/raw/conversaciones_final\n"
     ]
    }
   ],
   "source": [
    "# Extraemos la parte hablada\n",
    "limpiar_conversaciones()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b6d65e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminanos carpeta\n",
    "shutil.rmtree(\"data/raw/conversaciones_inicial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec73755",
   "metadata": {},
   "source": [
    "### Función para reconstruir lo hablado por un hablante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c24c5ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruir_lineas_conversacion():\n",
    "    # Carpetas de entrada y salida\n",
    "    carpeta_origen = \"data/raw/conversaciones_final\"\n",
    "    carpeta_destino = \"data/raw/conversaciones_finales\"\n",
    "    \n",
    "    if not os.path.exists(carpeta_destino):\n",
    "        os.makedirs(carpeta_destino)\n",
    "    \n",
    "    # Patrón para identificar el inicio de un turno:\n",
    "    # ^       -> Inicio de línea\n",
    "    # [A-Z]{1,2} -> Una o dos letras mayúsculas (ej: M, A, WC)\n",
    "    # \\)      -> Un paréntesis de cierre literal\n",
    "    patron_hablante = re.compile(r\"^[A-Z]{1,2}\\)\")\n",
    "\n",
    "    print(f\"Procesando archivos de '{carpeta_origen}'...\")\n",
    "    \n",
    "    archivos = sorted([f for f in os.listdir(carpeta_origen) if f.endswith(\".txt\")])\n",
    "\n",
    "    for nombre_archivo in archivos:\n",
    "        ruta_origen = os.path.join(carpeta_origen, nombre_archivo)\n",
    "        ruta_destino = os.path.join(carpeta_destino, nombre_archivo)\n",
    "        \n",
    "        with open(ruta_origen, 'r', encoding='utf-8') as f:\n",
    "            lineas_crudas = f.readlines()\n",
    "        \n",
    "        lineas_reconstruidas = []\n",
    "        buffer_turno = \"\" # Aquí acumularemos el texto de un turno completo\n",
    "        \n",
    "        for linea in lineas_crudas:\n",
    "            linea = linea.strip()\n",
    "            if not linea: continue # Saltar líneas vacías\n",
    "            \n",
    "            # Verificamos si la línea actual es el INICIO de un hablante\n",
    "            es_nuevo_turno = patron_hablante.match(linea)\n",
    "            \n",
    "            if es_nuevo_turno:\n",
    "                # Si ya teníamos un turno acumulado en el buffer, lo guardamos en la lista final\n",
    "                if buffer_turno:\n",
    "                    lineas_reconstruidas.append(buffer_turno)\n",
    "                \n",
    "                # Empezamos el nuevo turno\n",
    "                buffer_turno = linea\n",
    "            else:\n",
    "                # Si NO es un nuevo turno, es la continuación del anterior (la frase cortada)\n",
    "                # Lo unimos al buffer con un espacio\n",
    "                if buffer_turno:\n",
    "                    buffer_turno += \" \" + linea\n",
    "                else:\n",
    "                    # Caso borde: si el archivo empieza con una línea sin hablante (raro tras la limpieza)\n",
    "                    buffer_turno = linea\n",
    "        \n",
    "        # IMPORTANTE: Guardar el último turno que quedó en el buffer al terminar el bucle\n",
    "        if buffer_turno:\n",
    "            lineas_reconstruidas.append(buffer_turno)\n",
    "            \n",
    "        # Escribir el archivo reconstruido\n",
    "        with open(ruta_destino, 'w', encoding='utf-8') as f_out:\n",
    "            f_out.write('\\n'.join(lineas_reconstruidas))\n",
    "\n",
    "    print(f\"¡Proceso terminado! Se han reconstruido {len(archivos)} conversaciones.\")\n",
    "    print(f\"Los archivos listos están en: {carpeta_destino}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec75753e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivos de 'data/raw/conversaciones_final'...\n",
      "¡Proceso terminado! Se han reconstruido 40 conversaciones.\n",
      "Los archivos listos están en: data/raw/conversaciones_finales\n"
     ]
    }
   ],
   "source": [
    "# reconstruimos conversaciones\n",
    "reconstruir_lineas_conversacion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "341d1dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos carpeta\n",
    "shutil.rmtree(\"data/raw/conversaciones_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436fb961",
   "metadata": {},
   "source": [
    "### Función para crear corpus paralelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6450679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_corpus_final():\n",
    "    # Configuración de rutas\n",
    "    carpeta_entrada = \"data/raw/conversaciones_finales\"\n",
    "    carpeta_salida = \"data/raw/book\"\n",
    "    os.makedirs(carpeta_salida, exist_ok=True)\n",
    "    archivo_salida = os.path.join(carpeta_salida, \"book.tsv\")\n",
    "    \n",
    "    # Conjunto para guardar pares únicos (ayuda a eliminar duplicados automáticamente)\n",
    "    corpus_unico = set()\n",
    "    \n",
    "    # Compilamos el regex para quitar el hablante (Ej: \"M) \", \"R) \", \"AB) \")\n",
    "    # Busca al inicio de línea (^), 1 o 2 letras mayúsculas, un paréntesis y espacios opcionales\n",
    "    patron_hablante = re.compile(r\"^[A-Z]{1,2}\\)\\s*\")\n",
    "    \n",
    "    print(f\"Leyendo archivos de '{carpeta_entrada}'...\")\n",
    "    \n",
    "    # Listar archivos\n",
    "    if not os.path.exists(carpeta_entrada):\n",
    "        print(f\"Error: No existe la carpeta {carpeta_entrada}\")\n",
    "        return\n",
    "\n",
    "    archivos = [f for f in os.listdir(carpeta_entrada) if f.endswith(\".txt\")]\n",
    "    lineas_ignoradas_por_slash = 0\n",
    "    \n",
    "    for nombre_archivo in archivos:\n",
    "        ruta = os.path.join(carpeta_entrada, nombre_archivo)\n",
    "        \n",
    "        with open(ruta, 'r', encoding='utf-8') as f:\n",
    "            for linea in f:\n",
    "                linea = linea.strip()\n",
    "                if not linea: continue\n",
    "                \n",
    "                # REGLA 1: Verificar que tenga EXACTAMENTE un '/'\n",
    "                if linea.count('/') != 1:\n",
    "                    print(\"Linea rara --> \",linea)\n",
    "                    lineas_ignoradas_por_slash += 1\n",
    "                    continue\n",
    "                \n",
    "                # Dividir en Aimara (izquierda) y Español (derecha)\n",
    "                partes = linea.split('/')\n",
    "                parte_aimara_bruta = partes[0].strip()\n",
    "                parte_espanol_bruta = partes[1].strip()\n",
    "                \n",
    "                # REGLA 2: Quitar el identificador del hablante del lado Aimara\n",
    "                # Reemplazamos \"M) Hola\" por \"Hola\"\n",
    "                aimara_limpio = patron_hablante.sub('', parte_aimara_bruta)\n",
    "                \n",
    "                # REGLA 3: Convertir a minúsculas\n",
    "                aimara_final = aimara_limpio.lower().strip()\n",
    "                espanol_final = parte_espanol_bruta.lower().strip()\n",
    "                \n",
    "                # Validación extra: que no hayan quedado vacíos\n",
    "                if aimara_final and espanol_final:\n",
    "                    # REGLA 4: Al añadir a un set, se eliminan duplicados automáticamente\n",
    "                    corpus_unico.add((aimara_final, espanol_final))\n",
    "\n",
    "    # Ordenamos alfabéticamente para que el archivo quede ordenado\n",
    "    lista_corpus = sorted(list(corpus_unico))\n",
    "    \n",
    "    # Escribir el archivo TSV\n",
    "    print(f\"Escribiendo {len(lista_corpus)} pares únicos en '{archivo_salida}'...\")\n",
    "    \n",
    "    with open(archivo_salida, 'w', encoding='utf-8', newline='') as f_out:\n",
    "        # Usamos tabulador como separador\n",
    "        writer = csv.writer(f_out, delimiter='\\t')\n",
    "        \n",
    "        # Encabezado (opcional, útil para Pandas)\n",
    "        writer.writerow(['aymara', 'spanish'])\n",
    "        \n",
    "        # Escribir datos\n",
    "        writer.writerows(lista_corpus)\n",
    "        \n",
    "    print(\"-\" * 30)\n",
    "    print(\"¡Proceso Finalizado!\")\n",
    "    print(f\"Total de pares únicos generados: {len(lista_corpus)}\")\n",
    "    print(f\"Líneas descartadas por tener múltiples '/' o ninguno: {lineas_ignoradas_por_slash}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c430c10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivos de 'data/raw/conversaciones_finales'...\n",
      "Linea rara -->  Una mañana en la Terminal de Buses\n",
      "Linea rara -->  M) Ch’uqimpi ullukumpi, ch’arkhimpi, wali suma ulluk kaltu phayiristha / Con la papa, la papalisa y el charque, puedo cocinar un rico caldo de papalisa B: Kuna manq’as markaman wali uñyt’ata? / ¿Qué comida es la más conocida en tu pueblo?\n",
      "Linea rara -->  M) Kuna pachaxis jilata? / ¿Qué hora ya es hermano? B: Kimsaqalqu alwa chikatanixiwa / Son las 08:30 de la mañana A: Pachaxiw jilata, jutir phaxsin aruskiparakiñani, jichhax qulliriwkaru sarä, p’iqimpi, laka ch’akhampiw usutu / Ya es hora hermano, conversaremos el próximo mes, ahora tengo que ir donde el médico, me duele la cabeza y mi diente\n",
      "Linea rara -->  J) Jupax kuna luririsa? / ¿Cuál es la ocupación de él? A: Jupax jakhuriwa / Él es contador\n",
      "Linea rara -->  R) Nayax yatiqiritwa. Qhawqha jilatanitasa? Yo soy estudiante. ¿Cuántos hermanos tienes?\n",
      "Linea rara -->  R) Jumax kuna manq's munta? / ¿Qué comida quieres / te gusta?\n",
      "Linea rara -->  L) Akax tayka luk’anawa, akax wikuchir luk’ana, akax jilir luk’ana, akax surtij luk’ana, akaraki qallu luk’ana / Este es dedo pulgar, dedo índice, dedo mayor/medio, dedo anular y este dedo meñique\n",
      "Linea rara -->  L) Waliki Maribel / Que bien Maribel Lizetan chililiripaw chilinki / EL celular de Lizeth está sonando\n",
      "Escribiendo 1625 pares únicos en 'data/raw/book\\book.tsv'...\n",
      "------------------------------\n",
      "¡Proceso Finalizado!\n",
      "Total de pares únicos generados: 1625\n",
      "Líneas descartadas por tener múltiples '/' o ninguno: 8\n"
     ]
    }
   ],
   "source": [
    "# Creamos corpus paralelo\n",
    "generar_corpus_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b46bfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"data/raw/conversaciones_finales\") # eliminamos carpeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c233f9",
   "metadata": {},
   "source": [
    "## **Preprocesaiento del texto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d452c225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c49c21ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aymara</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1620</th>\n",
       "      <td>yuspajara, sarxa</td>\n",
       "      <td>gracias, me voy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>¡suma muxsarakisä uka manq’axa!</td>\n",
       "      <td>que rico es esa comida!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>¿juman sutimax kunasa?</td>\n",
       "      <td>¿cuál es tu nombre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>¿kullaka kunas puqhat sutimaxa?</td>\n",
       "      <td>¿cuál es tu nombre completo hermana?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>ñanqha uruwa</td>\n",
       "      <td>es viernes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               aymara                               spanish\n",
       "1620                 yuspajara, sarxa                       gracias, me voy\n",
       "1621  ¡suma muxsarakisä uka manq’axa!               que rico es esa comida!\n",
       "1622           ¿juman sutimax kunasa?                   ¿cuál es tu nombre?\n",
       "1623  ¿kullaka kunas puqhat sutimaxa?  ¿cuál es tu nombre completo hermana?\n",
       "1624                     ñanqha uruwa                            es viernes"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/raw/book/book.tsv\", sep=\"\\t\", encoding=\"utf-8\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bb5927f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '´', '¿', 'á', 'é', 'í', 'ñ', 'ó', 'ú', '’', '…']\n",
      "Cantidad de caracteres en español: 56\n"
     ]
    }
   ],
   "source": [
    "# Caracteres únicos del texto en Español\n",
    "spanish_chars = sorted(set(\"\".join(df[\"spanish\"].str.lower())))\n",
    "print(spanish_chars)\n",
    "print(\"Cantidad de caracteres en español:\", len(spanish_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11f10809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '!', \"'\", ',', '.', '3', '5', ':', '?', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '¡', '´', '¿', 'á', 'ä', 'é', 'í', 'ï', 'ñ', 'ó', 'ú', 'ü', '’', '…']\n",
      "Cantidad de caracteres en aymara: 50\n"
     ]
    }
   ],
   "source": [
    "# Caracteres únicos del texto en Aymara\n",
    "aymara_chars = sorted(set(\"\".join(df[\"aymara\"].str.lower())))\n",
    "print(aymara_chars)\n",
    "print(\"Cantidad de caracteres en aymara:\", len(aymara_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c45419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_unicode(text: str) -> str:\n",
    "    '''Para asegurarnos de que los caracteres se representen de una forma estándar interna'''\n",
    "    return unicodedata.normalize(\"NFC\", text)\n",
    "\n",
    "def normalize_quotes(text: str) -> str:\n",
    "    '''Para normalizar las comillas a comillas simples estándar ASCII'''\n",
    "    text = text.replace(\"«\", \"\\\"\").replace(\"»\", \"\\\"\")\n",
    "    text = text.replace(\"“\", \"\\\"\").replace(\"”\", \"\\\"\")\n",
    "    text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "    return text\n",
    "\n",
    "def normalize_punctuation(text: str) -> str:\n",
    "    '''Para normalizar signos de puntuación especiales'''\n",
    "    text = text.replace(\"—\", \"-\")\n",
    "    text = text.replace(\"…\", \"...\")\n",
    "    return text\n",
    "\n",
    "def clean_brackets(text: str) -> str:\n",
    "    '''Para eliminar los corchetes [ y ] del texto'''\n",
    "    return text.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "\n",
    "def remove_verse_numbers(text: str) -> str:\n",
    "    '''Cualquier número en el texto hace referencia a un código de versículo'''\n",
    "    words = text.split()\n",
    "    cleaned_words = [w for w in words if not any(ch.isdigit() for ch in w)] # Mantenemos solo palabras que no contienen dígitos\n",
    "    return \" \".join(cleaned_words)\n",
    "\n",
    "def clean_spaces(text: str) -> str:\n",
    "    '''Para normalizar los espacios en el texto'''\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def lowercase(text: str) -> str:\n",
    "    '''Para convertir todo el texto a minúsculas'''\n",
    "    return text.lower()\n",
    "\n",
    "def preprocess(text: str) -> str:\n",
    "    text = normalize_unicode(text)\n",
    "    text = normalize_quotes(text)\n",
    "    text = normalize_punctuation(text)\n",
    "    text = clean_brackets(text)\n",
    "    # text = remove_verse_numbers(text) # Mantenemos los números\n",
    "    text = clean_spaces(text)\n",
    "    text = lowercase(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10668125",
   "metadata": {},
   "source": [
    "### Aplicamos preprocesamiento a book.tsv para obtener book/spanish.txt y book/aymara.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a04060c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos data/clean/book/spanish.txt y data/clean/book/aymara.txt generados exitosamente...\n"
     ]
    }
   ],
   "source": [
    "raw_corpus = \"data/raw/book/book.tsv\"\n",
    "folder_clean_path = \"data/clean/book\"\n",
    "os.makedirs(folder_clean_path, exist_ok=True)\n",
    "output_spanish_clean = f\"{folder_clean_path}/spanish.txt\"\n",
    "output_aymara_clean = f\"{folder_clean_path}/aymara.txt\"\n",
    "\n",
    "spanish_lines = []\n",
    "aymara_lines = []\n",
    "\n",
    "with open(raw_corpus, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\"\\t\")\n",
    "    \n",
    "    for row in reader:\n",
    "        spanish_raw = row[\"spanish\"]\n",
    "        aymara_raw = row[\"aymara\"]\n",
    "\n",
    "        spanish_clean = preprocess(spanish_raw)\n",
    "        aymara_clean = preprocess(aymara_raw)\n",
    "\n",
    "        if not spanish_clean or not aymara_clean:\n",
    "            continue\n",
    "        \n",
    "        spanish_lines.append(spanish_clean)\n",
    "        aymara_lines.append(aymara_clean)\n",
    "\n",
    "# Guardamos los resultados\n",
    "with open(output_spanish_clean, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in spanish_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "with open(output_aymara_clean, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in aymara_lines:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"Archivos {output_spanish_clean} y {output_aymara_clean} generados exitosamente...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
