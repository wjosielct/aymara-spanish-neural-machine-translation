{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7374ee3",
   "metadata": {},
   "source": [
    "# **Entrenamiento Aymara $\\rightarrow$ Español**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7cb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentencepiece\n",
    "!pip install -q sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ad4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.amp import autocast, GradScaler\n",
    "import sentencepiece as spm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "import os\n",
    "import sacrebleu\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4766668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. DATASET Y DATALOADER\n",
    "# ==========================================\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_path, trgt_path):\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.src_lines = [line.strip() for line in f]\n",
    "        with open(trgt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.trgt_lines = [line.strip() for line in f]\n",
    "        assert len(self.src_lines) == len(self.trgt_lines)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = list(map(int, self.src_lines[idx].split()))\n",
    "        trgt = list(map(int, self.trgt_lines[idx].split()))\n",
    "        # Aseguramos long para embeddings\n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(trgt, dtype=torch.long)\n",
    "\n",
    "train_dataset = TranslationDataset(src_path=\"data/splits/ids/train.aymara\", trgt_path=\"data/splits/ids/train.spanish\")\n",
    "valid_dataset = TranslationDataset(src_path=\"data/splits/ids/valid.aymara\", trgt_path=\"data/splits/ids/valid.spanish\")\n",
    "test_dataset  = TranslationDataset(src_path=\"data/splits/ids/test.aymara\",  trgt_path=\"data/splits/ids/test.spanish\")\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tokenizer/SentencePiece.model\")\n",
    "PAD_ID, BOS_ID, EOS_ID = sp.pad_id(), sp.bos_id(), sp.eos_id()\n",
    "vocab_size = sp.vocab_size()\n",
    "\n",
    "def collate_fn_batch(batch):\n",
    "    src_batch, trgt_batch = zip(*batch)\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=PAD_ID)\n",
    "    trgt_padded = pad_sequence(trgt_batch, batch_first=True, padding_value=PAD_ID)\n",
    "    return src_padded, trgt_padded\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_batch)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_batch)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5305004a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. TRANSFORMER\n",
    "# ==========================================\n",
    "\n",
    "class TranslationTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, n_heads=8, num_layers=6, dim_ffnn=2048, dropout=0.2, pad_id=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model # Guardamos d_model para el escalado\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.positional_encoder = nn.Embedding(150, d_model) # 150 es seguro para max_len 80\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=dim_ffnn,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=d_model, out_features=vocab_size, bias=False)\n",
    "        self.output_layer.weight = self.embedding.weight \n",
    "\n",
    "    def forward(self, src, trgt, trgt_causal_mask=None, src_key_padding_mask=None, trgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        src_pos = self.positional_encoder(torch.arange(src.size(1), device=src.device))\n",
    "        trgt_pos = self.positional_encoder(torch.arange(trgt.size(1), device=trgt.device))\n",
    "\n",
    "        # Escalado por sqrt(d_model)\n",
    "        src = (self.embedding(src) * math.sqrt(self.d_model)) + src_pos\n",
    "        trgt = (self.embedding(trgt) * math.sqrt(self.d_model)) + trgt_pos\n",
    "\n",
    "        out = self.transformer(\n",
    "            src,\n",
    "            trgt,\n",
    "            tgt_mask=trgt_causal_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=trgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        return self.output_layer(out)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07eef9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. INICIALIZACIÓN\n",
    "# ==========================================\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "model = TranslationTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    num_layers=6,\n",
    "    dim_ffnn=2048,\n",
    "    dropout=0.1,\n",
    "    pad_id=PAD_ID\n",
    ").to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.0005, total_steps=total_steps, pct_start=0.1, anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "700d053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. FUNCIONES DE ENTRENAMIENTO Y DECODING\n",
    "# ==========================================\n",
    "\n",
    "def create_padding_mask(batch, pad_id):\n",
    "    return (batch == pad_id)\n",
    "\n",
    "def create_causal_mask(size, device):\n",
    "    return nn.Transformer.generate_square_subsequent_mask(size).to(device)\n",
    "\n",
    "def train_epoch_amp(model, data_loader, optimizer, criterion, device, pad_id, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for src, trgt in data_loader:\n",
    "        src, trgt = src.to(device), trgt.to(device)\n",
    "        trgt_in, trgt_out = trgt[:, :-1], trgt[:, 1:].contiguous()\n",
    "\n",
    "        trgt_causal_mask = create_causal_mask(trgt_in.size(1), device)\n",
    "        src_mask = create_padding_mask(src, pad_id)\n",
    "        trgt_mask = create_padding_mask(trgt_in, pad_id)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            logits = model(src, trgt_in, trgt_causal_mask, src_mask, trgt_mask, src_mask)\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), trgt_out.reshape(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, data_loader, criterion, device, pad_id):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for src, trgt in data_loader:\n",
    "        src, trgt = src.to(device), trgt.to(device)\n",
    "        trgt_in, trgt_out = trgt[:, :-1], trgt[:, 1:].contiguous()\n",
    "        \n",
    "        trgt_causal_mask = create_causal_mask(trgt_in.size(1), device)\n",
    "        src_mask, trgt_mask = create_padding_mask(src, pad_id), create_padding_mask(trgt_in, pad_id)\n",
    "\n",
    "        logits = model(src, trgt_in, trgt_causal_mask, src_mask, trgt_mask, src_mask)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), trgt_out.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_search_decode(model, src, sp, beam_size, max_len, device):\n",
    "    PAD_ID, BOS_ID, EOS_ID = sp.pad_id(), sp.bos_id(), sp.eos_id()\n",
    "    src = src.unsqueeze(0).to(device)\n",
    "    src_mask = create_padding_mask(src, PAD_ID).to(device)\n",
    "    \n",
    "    # Encoding manual debe considerar el escalado sqrt(d_model)\n",
    "    src_emb = model.embedding(src) * math.sqrt(model.d_model)\n",
    "    src_pos = model.positional_encoder(torch.arange(src.size(1), device=device))\n",
    "    memory = model.transformer.encoder(src_emb + src_pos, src_key_padding_mask=src_mask)\n",
    "\n",
    "    candidates = [([BOS_ID], 0.0)]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        new_candidates = []\n",
    "        all_finished = True\n",
    "        \n",
    "        for seq, score in candidates:\n",
    "            if seq[-1] == EOS_ID:\n",
    "                new_candidates.append((seq, score))\n",
    "                continue\n",
    "            \n",
    "            all_finished = False\n",
    "            trgt_in = torch.tensor([seq], device=device)\n",
    "            \n",
    "            # Decoder forward manual también con escalado\n",
    "            trgt_emb = model.embedding(trgt_in) * math.sqrt(model.d_model)\n",
    "            trgt_pos = model.positional_encoder(torch.arange(trgt_in.size(1), device=device))\n",
    "            \n",
    "            trgt_causal_mask = create_causal_mask(trgt_in.size(1), device)\n",
    "            out = model.transformer.decoder(trgt_emb + trgt_pos, memory, tgt_mask=trgt_causal_mask, memory_key_padding_mask=src_mask)\n",
    "            \n",
    "            log_probs = torch.log_softmax(model.output_layer(out[:, -1, :]), dim=-1)\n",
    "            topk_probs, topk_ids = torch.topk(log_probs, beam_size, dim=-1)\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                new_candidates.append((seq + [topk_ids[0][i].item()], score + topk_probs[0][i].item()))\n",
    "        \n",
    "        if all_finished: break\n",
    "        candidates = sorted(new_candidates, key=lambda x: x[1] / (len(x[0]) ** 0.7), reverse=True)[:beam_size]\n",
    "        \n",
    "    return candidates[0][0]\n",
    "\n",
    "def bleu_epoch(model, dataset, sp, device, beam_size=3, max_len=80, limit=1000):\n",
    "    model.eval()\n",
    "    hypotheses, references = [], []\n",
    "    \n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    if limit is None:\n",
    "        n_eval = total_samples # Si es None, usamos todo el data set\n",
    "    else:\n",
    "        n_eval = min(total_samples, limit)\n",
    "        \n",
    "    indices = range(n_eval)\n",
    "    # ------------------\n",
    "    \n",
    "    if n_eval > 1000:\n",
    "        print(f\"Evaluando {n_eval} oraciones (esto tomará un tiempo)...\")\n",
    "\n",
    "    for idx in indices:\n",
    "        src, trgt = dataset[idx]\n",
    "        references.append(sp.decode(trgt.tolist()))\n",
    "        hypotheses.append(sp.decode(beam_search_decode(model, src, sp, beam_size, max_len, device)))\n",
    "        \n",
    "    return sacrebleu.corpus_bleu(hypotheses, [references]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3753cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando 100 épocas. Validando BLEU cada 10 épocas (max 1000 oraciones).\n",
      "Epoch 1 | LR: 0.00003175 | Train Loss: 6.7083 | Val Loss: 5.8046 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 2 | LR: 0.00006584 | Train Loss: 5.5194 | Val Loss: 5.2070 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 3 | LR: 0.00011895 | Train Loss: 5.0014 | Val Loss: 4.7113 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 4 | LR: 0.00018586 | Train Loss: 4.4889 | Val Loss: 4.1720 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 5 | LR: 0.00026003 | Train Loss: 3.9721 | Val Loss: 3.7414 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 6 | LR: 0.00033420 | Train Loss: 3.5991 | Val Loss: 3.4669 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 7 | LR: 0.00040111 | Train Loss: 3.3404 | Val Loss: 3.2690 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 8 | LR: 0.00045419 | Train Loss: 3.1517 | Val Loss: 3.1382 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 9 | LR: 0.00048827 | Train Loss: 3.0021 | Val Loss: 3.0181 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "--> ¡Nuevo récord BLEU: 20.49!\n",
      "Epoch 10 | LR: 0.00050000 | Train Loss: 2.8726 | Val Loss: 2.9222 | Val BLEU: 20.49\n",
      "------------------------------------------------------------\n",
      "Epoch 11 | LR: 0.00049985 | Train Loss: 2.7500 | Val Loss: 2.8460 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 12 | LR: 0.00049939 | Train Loss: 2.6414 | Val Loss: 2.7659 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 13 | LR: 0.00049863 | Train Loss: 2.5515 | Val Loss: 2.6964 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 14 | LR: 0.00049757 | Train Loss: 2.4751 | Val Loss: 2.6492 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 15 | LR: 0.00049620 | Train Loss: 2.4106 | Val Loss: 2.6056 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 16 | LR: 0.00049454 | Train Loss: 2.3534 | Val Loss: 2.5698 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 17 | LR: 0.00049257 | Train Loss: 2.3050 | Val Loss: 2.5228 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 18 | LR: 0.00049031 | Train Loss: 2.2625 | Val Loss: 2.4897 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 19 | LR: 0.00048776 | Train Loss: 2.2252 | Val Loss: 2.4648 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "--> ¡Nuevo récord BLEU: 28.25!\n",
      "Epoch 20 | LR: 0.00048492 | Train Loss: 2.1872 | Val Loss: 2.4356 | Val BLEU: 28.25\n",
      "------------------------------------------------------------\n",
      "Epoch 21 | LR: 0.00048179 | Train Loss: 2.1552 | Val Loss: 2.4155 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 22 | LR: 0.00047838 | Train Loss: 2.1256 | Val Loss: 2.3929 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 23 | LR: 0.00047470 | Train Loss: 2.0973 | Val Loss: 2.3701 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 24 | LR: 0.00047073 | Train Loss: 2.0721 | Val Loss: 2.3563 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 25 | LR: 0.00046650 | Train Loss: 2.0473 | Val Loss: 2.3336 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 26 | LR: 0.00046201 | Train Loss: 2.0243 | Val Loss: 2.3241 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 27 | LR: 0.00045726 | Train Loss: 2.0060 | Val Loss: 2.3042 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 28 | LR: 0.00045225 | Train Loss: 1.9862 | Val Loss: 2.2783 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 29 | LR: 0.00044700 | Train Loss: 1.9638 | Val Loss: 2.2661 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "--> ¡Nuevo récord BLEU: 30.17!\n",
      "Epoch 30 | LR: 0.00044151 | Train Loss: 1.9437 | Val Loss: 2.2480 | Val BLEU: 30.17\n",
      "------------------------------------------------------------\n",
      "Epoch 31 | LR: 0.00043578 | Train Loss: 1.9251 | Val Loss: 2.2375 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 32 | LR: 0.00042983 | Train Loss: 1.9072 | Val Loss: 2.2230 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 33 | LR: 0.00042366 | Train Loss: 1.8895 | Val Loss: 2.2116 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 34 | LR: 0.00041728 | Train Loss: 1.8741 | Val Loss: 2.1920 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 35 | LR: 0.00041069 | Train Loss: 1.8572 | Val Loss: 2.1799 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 36 | LR: 0.00040391 | Train Loss: 1.8410 | Val Loss: 2.1702 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 37 | LR: 0.00039694 | Train Loss: 1.8258 | Val Loss: 2.1603 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 38 | LR: 0.00038979 | Train Loss: 1.8124 | Val Loss: 2.1539 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 39 | LR: 0.00038247 | Train Loss: 1.7978 | Val Loss: 2.1272 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 40 | LR: 0.00037499 | Train Loss: 1.7831 | Val Loss: 2.1202 | Val BLEU: 29.73\n",
      "------------------------------------------------------------\n",
      "Epoch 41 | LR: 0.00036736 | Train Loss: 1.7687 | Val Loss: 2.1155 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 42 | LR: 0.00035959 | Train Loss: 1.7556 | Val Loss: 2.1043 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 43 | LR: 0.00035168 | Train Loss: 1.7417 | Val Loss: 2.0877 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 44 | LR: 0.00034365 | Train Loss: 1.7300 | Val Loss: 2.0813 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 45 | LR: 0.00033550 | Train Loss: 1.7172 | Val Loss: 2.0758 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 46 | LR: 0.00032725 | Train Loss: 1.7051 | Val Loss: 2.0632 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 47 | LR: 0.00031890 | Train Loss: 1.6942 | Val Loss: 2.0434 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 48 | LR: 0.00031047 | Train Loss: 1.6817 | Val Loss: 2.0389 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 49 | LR: 0.00030197 | Train Loss: 1.6706 | Val Loss: 2.0319 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 50 | LR: 0.00029341 | Train Loss: 1.6598 | Val Loss: 2.0248 | Val BLEU: 29.08\n",
      "------------------------------------------------------------\n",
      "Epoch 51 | LR: 0.00028479 | Train Loss: 1.6482 | Val Loss: 2.0185 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 52 | LR: 0.00027613 | Train Loss: 1.6383 | Val Loss: 2.0082 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 53 | LR: 0.00026743 | Train Loss: 1.6283 | Val Loss: 2.0026 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 54 | LR: 0.00025872 | Train Loss: 1.6182 | Val Loss: 1.9905 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 55 | LR: 0.00024999 | Train Loss: 1.6090 | Val Loss: 1.9835 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 56 | LR: 0.00024127 | Train Loss: 1.5986 | Val Loss: 1.9809 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 57 | LR: 0.00023255 | Train Loss: 1.5902 | Val Loss: 1.9739 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 58 | LR: 0.00022386 | Train Loss: 1.5812 | Val Loss: 1.9610 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 59 | LR: 0.00021520 | Train Loss: 1.5725 | Val Loss: 1.9609 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 60 | LR: 0.00020658 | Train Loss: 1.5646 | Val Loss: 1.9539 | Val BLEU: 27.93\n",
      "------------------------------------------------------------\n",
      "Epoch 61 | LR: 0.00019802 | Train Loss: 1.5565 | Val Loss: 1.9491 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 62 | LR: 0.00018951 | Train Loss: 1.5487 | Val Loss: 1.9422 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 63 | LR: 0.00018108 | Train Loss: 1.5409 | Val Loss: 1.9368 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 64 | LR: 0.00017274 | Train Loss: 1.5340 | Val Loss: 1.9316 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 65 | LR: 0.00016449 | Train Loss: 1.5267 | Val Loss: 1.9290 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 66 | LR: 0.00015634 | Train Loss: 1.5200 | Val Loss: 1.9196 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 67 | LR: 0.00014831 | Train Loss: 1.5134 | Val Loss: 1.9185 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 68 | LR: 0.00014040 | Train Loss: 1.5070 | Val Loss: 1.9155 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 69 | LR: 0.00013263 | Train Loss: 1.5015 | Val Loss: 1.9087 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 70 | LR: 0.00012499 | Train Loss: 1.4954 | Val Loss: 1.9082 | Val BLEU: 26.86\n",
      "------------------------------------------------------------\n",
      "Epoch 71 | LR: 0.00011752 | Train Loss: 1.4897 | Val Loss: 1.9009 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 72 | LR: 0.00011020 | Train Loss: 1.4844 | Val Loss: 1.9001 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 73 | LR: 0.00010305 | Train Loss: 1.4791 | Val Loss: 1.8956 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 74 | LR: 0.00009608 | Train Loss: 1.4746 | Val Loss: 1.8925 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 75 | LR: 0.00008930 | Train Loss: 1.4699 | Val Loss: 1.8942 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 76 | LR: 0.00008271 | Train Loss: 1.4654 | Val Loss: 1.8902 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 77 | LR: 0.00007633 | Train Loss: 1.4614 | Val Loss: 1.8874 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 78 | LR: 0.00007016 | Train Loss: 1.4573 | Val Loss: 1.8859 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 79 | LR: 0.00006421 | Train Loss: 1.4538 | Val Loss: 1.8815 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 80 | LR: 0.00005849 | Train Loss: 1.4501 | Val Loss: 1.8844 | Val BLEU: 27.01\n",
      "------------------------------------------------------------\n",
      "Epoch 81 | LR: 0.00005299 | Train Loss: 1.4470 | Val Loss: 1.8836 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 82 | LR: 0.00004774 | Train Loss: 1.4438 | Val Loss: 1.8805 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 83 | LR: 0.00004274 | Train Loss: 1.4408 | Val Loss: 1.8794 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 84 | LR: 0.00003799 | Train Loss: 1.4383 | Val Loss: 1.8796 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 85 | LR: 0.00003349 | Train Loss: 1.4359 | Val Loss: 1.8764 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 86 | LR: 0.00002926 | Train Loss: 1.4336 | Val Loss: 1.8774 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 87 | LR: 0.00002530 | Train Loss: 1.4313 | Val Loss: 1.8762 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 88 | LR: 0.00002161 | Train Loss: 1.4294 | Val Loss: 1.8761 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 89 | LR: 0.00001820 | Train Loss: 1.4277 | Val Loss: 1.8778 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 90 | LR: 0.00001508 | Train Loss: 1.4265 | Val Loss: 1.8780 | Val BLEU: 26.99\n",
      "------------------------------------------------------------\n",
      "Epoch 91 | LR: 0.00001224 | Train Loss: 1.4251 | Val Loss: 1.8776 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 92 | LR: 0.00000968 | Train Loss: 1.4238 | Val Loss: 1.8782 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 93 | LR: 0.00000743 | Train Loss: 1.4231 | Val Loss: 1.8773 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 94 | LR: 0.00000546 | Train Loss: 1.4222 | Val Loss: 1.8774 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 95 | LR: 0.00000380 | Train Loss: 1.4214 | Val Loss: 1.8768 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 96 | LR: 0.00000243 | Train Loss: 1.4207 | Val Loss: 1.8772 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 97 | LR: 0.00000137 | Train Loss: 1.4205 | Val Loss: 1.8772 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 98 | LR: 0.00000061 | Train Loss: 1.4202 | Val Loss: 1.8776 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 99 | LR: 0.00000015 | Train Loss: 1.4199 | Val Loss: 1.8774 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 100 | LR: 0.00000000 | Train Loss: 1.4199 | Val Loss: 1.8774 | Val BLEU: 26.88\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. LOOP PRINCIPAL\n",
    "# ==========================================\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_bleu = 0.0\n",
    "training_history = []\n",
    "\n",
    "print(f\"Entrenando {EPOCHS} épocas. Validando BLEU cada 10 épocas (max 1000 oraciones).\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_epoch_amp(model, train_loader, optimizer, criterion, device, PAD_ID, scheduler)\n",
    "    val_loss = eval_epoch(model, valid_loader, criterion, device, PAD_ID)\n",
    "    val_ppl = math.exp(min(val_loss, 100))\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"models/best_loss_model_AymaraToSpanish_Standard.pt\")\n",
    "\n",
    "    bleu_score = None\n",
    "    if epoch % 10 == 0 or epoch == EPOCHS:\n",
    "        print(\"--> Calculando BLEU...\")\n",
    "        # Usamos beam_size=3 para calidad\n",
    "        bleu_score = bleu_epoch(model, valid_dataset, sp, device, beam_size=3, max_len=80, limit=1000)\n",
    "        \n",
    "        if bleu_score > best_bleu:\n",
    "            best_bleu = bleu_score\n",
    "            torch.save(model.state_dict(), \"models/best_bleu_model_AymaraToSpanish_Standard.pt\")\n",
    "            print(f\"--> ¡Nuevo récord BLEU: {best_bleu:.2f}!\")\n",
    "\n",
    "    bleu_str = f\"{bleu_score:.2f}\" if bleu_score is not None else \"-\"\n",
    "    print(f\"Epoch {epoch} | LR: {current_lr:.8f} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val BLEU: {bleu_str}\")\n",
    "\n",
    "    training_history.append({\n",
    "        \"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss,\n",
    "        \"val_ppl\": val_ppl, \"val_bleu\": bleu_score, \"lr\": current_lr\n",
    "    })\n",
    "    pd.DataFrame(training_history).to_csv(\"results/metrics_AymaraToSpanish_Standard.csv\", index=False)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d837a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el mejor modelo para evaluación del conjunto de prueba...\n",
      "--> Cargando el mejor modelo guardado: models/best_bleu_model_AymaratoSpanish.pt\n",
      "Pesos del modelo cargados exitosamente!\n",
      "\n",
      "Calculando métricas en el Test Set completo...\n",
      "Evaluando 8187 oraciones (esto tomará un tiempo)...\n",
      "\n",
      "Resultados Finales del Test Set:\n",
      "===================================\n",
      "Test Loss: 2.2585\n",
      "Test PPL : 9.57\n",
      "Test BLEU: 28.95 (Beam=5)\n",
      "===================================\n",
      "\n",
      "--- Ejemplos de Traducción ---\n",
      "Aymara:  taqi chuymaw taqi akanak yatxatañar uchasta, aski jaqinakana, yatiñ kankañan jaqinakan luratanakapan diosan amparapankatap yatiñataki. jaqix munasiñxatsa, uñisisiñxatsa, ukanakan uñjkasas janiw kuns yatkiti.\n",
      "Español: a todo esto me entregué de lleno, tan solo para descubrir que aunque las obras de buenos y de sabios están en las manos de dios, ellos no saben si dios los acepta o los rechaza, sino que todo descansa en el futuro.\n",
      "Modelo:  con todo, me entregué de lleno a investigar y estudiar con sabiduría todo cuanto se hace en favor de justos, para conocer en las obras de dios y en las manos de los hombres, y para que el hombre no sepa nada de lo que hace.\n",
      "--------------------------------------------------\n",
      "Aymara:  ¿cunarac jiwatajjamp jicjjatasmasti?, ¿cunarac sepulturar mantatajjampist apsusmasti? lak'ajj janiw yuspagarquiristamti, janiraquiw asqui cancañamats parlcaspati.\n",
      "Español: ¿qué se gana con que yo muera, con que sea llevado al sepulcro? ¡el polvo no puede alabarte ni hablar de tu fidelidad!\n",
      "Modelo:  ¿qué provecho te alcanzar con la muerte y con qué puedo entrar en el sepulcro? pues si doy gracias a mi boca, no puede hablar de tu verdad.\n",
      "--------------------------------------------------\n",
      "Aymara:  uca altar amsta patarusa, puspach thiyanacarusa, wajjranacarusa korimpiwa lip'suyaraquïta, uqhamarac thiyanacatsa korimpiraquiw ucharaquïtajja.\n",
      "Español: recubre de oro puro su parte superior, sus cuatro costados y sus cuernos, y ponle un ribete de oro alrededor.\n",
      "Modelo:  lo recubrirás de oro puro, tanto su cubierta como sus paredes alrededor y sus cuernos. le harás alrededor una moldura de oro.\n",
      "--------------------------------------------------\n",
      "Aymara:  ucampisa jupanacatejj: 'maqhatanipjjam' sistani ucapachasti, maqhatañäniwa, ucasti jiwasataquejj mä unanchäniwa, tatitun jiwasaru atipjäwi jupanacjjaru churañapataqui.\n",
      "Español: pero si nos dicen así: \"suban hasta nosotros\", entonces subiremos; porque el señor los ha entregado en nuestra mano. esta será la señal para nosotros.\n",
      "Modelo:  pero si nos dicen que subamos, lo haremos así, porque eso será una señal de que podremos vencernos.\n",
      "--------------------------------------------------\n",
      "Aymara:  akanak arsutaxax p'inqasiykchitu ukasa, chiqpachansa nanakax jumanakatakix sinti alt'at chuymaniw tukupxäyätxa, uka jaqinakjama jumanakaru lurañatakixa.\n",
      "Español: con vergüenza lo digo, como que hemos sido débiles.\n",
      "Modelo:  con vergüenza lo que he dicho, pero con nuestra deshonra se ha hecho demasiado malo por causa de ustedes.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. EVALUACIÓN FINAL EN TEST SET\n",
    "# ==========================================\n",
    "\n",
    "print(\"Cargando el mejor modelo para evaluación del conjunto de prueba...\")\n",
    "\n",
    "# 1. Re-inicializamos el modelo limpio\n",
    "model_test = TranslationTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    num_layers=6,\n",
    "    dim_ffnn=2048,\n",
    "    dropout=0.0, # Dropout no se usa en inferencia\n",
    "    pad_id=PAD_ID\n",
    ").to(device)\n",
    "\n",
    "# 2. Cargas los pesos. \n",
    "model_path = \"models/best_bleu_model_AymaraToSpanish_Standard.pt\"\n",
    "print(f\"--> Cargando el mejor modelo guardado: {model_path}\")\n",
    "model_test.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(\"Pesos del modelo cargados exitosamente!\")\n",
    "\n",
    "model_test.eval()\n",
    "\n",
    "# ------------------------------------------\n",
    "# A. Cálculo de Métricas Cuantitativas\n",
    "# ------------------------------------------\n",
    "print(\"\\nCalculando métricas en el Test Set completo...\")\n",
    "\n",
    "# 1. Loss y Perplexity\n",
    "test_loss = eval_epoch(model_test, test_loader, criterion, device, PAD_ID)\n",
    "test_ppl = math.exp(min(test_loss, 100))\n",
    "\n",
    "# 2. BLEU Score\n",
    "# Nota: Ponemos limit=None para evaluar el test set completo\n",
    "# Nota: Beam size=5 es estándar para publicaciones y resultados finales\n",
    "test_bleu = bleu_epoch(model_test, test_dataset, sp, device, beam_size=5, max_len=80, limit=None)\n",
    "\n",
    "print(f\"\\nResultados Finales del Test Set:\")\n",
    "print(f\"===================================\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test PPL : {test_ppl:.2f}\")\n",
    "print(f\"Test BLEU: {test_bleu:.2f} (Beam=5)\")\n",
    "print(f\"===================================\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# B. Evaluación Cualitativa (Ver traducciones)\n",
    "# ------------------------------------------\n",
    "print(\"\\n--- Ejemplos de Traducción ---\")\n",
    "indices = random.sample(range(len(test_dataset)), 5) # 5 ejemplos al azar\n",
    "\n",
    "for idx in indices:\n",
    "    src, trgt = test_dataset[idx]\n",
    "    \n",
    "    src_text = sp.decode(src.tolist())\n",
    "    trgt_text = sp.decode(trgt.tolist())\n",
    "    \n",
    "    # Inferencia\n",
    "    pred_ids = beam_search_decode(model_test, src, sp, beam_size=5, max_len=80, device=device)\n",
    "    pred_text = sp.decode(pred_ids)\n",
    "    \n",
    "    print(f\"Aymara:  {src_text}\")\n",
    "    print(f\"Español: {trgt_text}\")\n",
    "    print(f\"Modelo:  {pred_text}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc9723-71f8-4ca6-82f2-78c098a999c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
