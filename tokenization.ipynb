{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9114cd4",
   "metadata": {},
   "source": [
    "## **Entrenamiento del tokenizador basado en sub-palabras**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d019103d",
   "metadata": {},
   "source": [
    "### Librerías a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a374ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde2d56c",
   "metadata": {},
   "source": [
    "### Creamos archivo combinado: spanish + aymara --> combined.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067c4bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo data/clean/spanish.txt guardado exitosamente ...\n"
     ]
    }
   ],
   "source": [
    "bible_spanish_file_clean = \"data/clean/bible/spanish.txt\"\n",
    "book_spanish_file_clean = \"data/clean/book/spanish.txt\"\n",
    "combined_file = \"data/clean/spanish.txt\"\n",
    "\n",
    "with open(combined_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    with open(bible_spanish_file_clean, \"r\", encoding=\"utf-8\") as f_bible_spanish:\n",
    "        out.write(f_bible_spanish.read())\n",
    "    \n",
    "    with open(book_spanish_file_clean, \"r\", encoding=\"utf-8\") as f_book_spanish:\n",
    "        out.write(f_book_spanish.read())\n",
    "\n",
    "print(f\"Archivo {combined_file} guardado exitosamente ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67cf3217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo data/clean/aymara.txt guardado exitosamente ...\n"
     ]
    }
   ],
   "source": [
    "bible_aymara_file_clean = \"data/clean/bible/aymara.txt\"\n",
    "book_aymara_file_clean = \"data/clean/book/aymara.txt\"\n",
    "combined_file = \"data/clean/aymara.txt\"\n",
    "\n",
    "with open(combined_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    with open(bible_aymara_file_clean, \"r\", encoding=\"utf-8\") as f_bible_aymara:\n",
    "        out.write(f_bible_aymara.read())\n",
    "    \n",
    "    with open(book_aymara_file_clean, \"r\", encoding=\"utf-8\") as f_book_aymara:\n",
    "        out.write(f_book_aymara.read())\n",
    "\n",
    "print(f\"Archivo {combined_file} guardado exitosamente ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f47441f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo data/clean/combined.txt guardado exitosamente ...\n"
     ]
    }
   ],
   "source": [
    "spanish_file_clean = \"data/clean/spanish.txt\"\n",
    "aymara_file_clean = \"data/clean/aymara.txt\"\n",
    "combined_file = \"data/clean/combined.txt\"\n",
    "\n",
    "with open(combined_file, \"w\", encoding=\"utf-8\") as out:\n",
    "    with open(spanish_file_clean, \"r\", encoding=\"utf-8\") as f_spanish:\n",
    "        out.write(f_spanish.read())\n",
    "    \n",
    "    with open(aymara_file_clean, \"r\", encoding=\"utf-8\") as f_aymara:\n",
    "        out.write(f_aymara.read())\n",
    "\n",
    "print(f\"Archivo {combined_file} guardado exitosamente ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b84a5",
   "metadata": {},
   "source": [
    "### Entrenamiento del tokenizador SentencePiece (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c70bbd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"tokenizer\", exist_ok=True) # Creamos carpeta si no existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b2369aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_file = \"data/clean/combined.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "946433a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spm.SentencePieceTrainer.Train(\n",
    "    input=combined_file,\n",
    "    model_prefix=\"tokenizer/SentencePiece\",\n",
    "    vocab_size=16000,\n",
    "    model_type=\"bpe\",\n",
    "    character_coverage=1.0, # 100% de caracteres del texto debe ser reconocido por el tokenizer\n",
    "    shuffle_input_sentence=True,\n",
    "    normalization_rule_name=\"nmt_nfkc\",\n",
    "    pad_id=0,\n",
    "    bos_id=1,\n",
    "    eos_id=2,\n",
    "    unk_id=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0d7ac3",
   "metadata": {},
   "source": [
    "## **Verificación del tokenizador SentencePiece entrenado**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8ee1d",
   "metadata": {},
   "source": [
    "### Librerías a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f305891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ae7a4",
   "metadata": {},
   "source": [
    "### Tokens especiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b512af2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tokenizer/SentencePiece.model\") # cargamos el tokenizador entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e70aa03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_id --> 0\n",
      "bos_id --> 1\n",
      "eos_id --> 2\n",
      "unk_id --> 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"pad_id --> {sp.pad_id()}\")\n",
    "print(f\"bos_id --> {sp.bos_id()}\")\n",
    "print(f\"eos_id --> {sp.eos_id()}\")\n",
    "print(f\"unk_id --> {sp.unk_id()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030a7d75",
   "metadata": {},
   "source": [
    "### Probando el tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15d86a1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (ids): [231, 46, 25, 571, 15958, 551, 7931, 15958, 5794, 10296, 29, 7316, 703, 15981]\n",
      "Tokens (Subwords): ['▁¡', 'ay', '▁de', '▁jerusalén', ',', '▁ciudad', '▁rebelde', ',', '▁contam', 'inada', '▁y', '▁opres', 'ora', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"¡ay de jerusalén, ciudad rebelde, contaminada y opresora!\"\n",
    "print(\"Tokens (ids):\", sp.encode(text, out_type=int))\n",
    "print(\"Tokens (Subwords):\", sp.encode(text, out_type=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db32c00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡ay de jerusalén, ciudad rebelde, contaminada y opresora!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Los tokens BOS, EOS y PAD se ignoran al decodificar\n",
    "sp.decode([1] + sp.encode(text, out_type=int) + [2] + [0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb39866",
   "metadata": {},
   "source": [
    "## **Tokenización del corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09577551",
   "metadata": {},
   "source": [
    "### Librerías a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37795f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3989a5",
   "metadata": {},
   "source": [
    "### Cargamos tokenizador entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76e5681e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizador cargado exitosamente...\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tokenizer/SentencePiece.model\")\n",
    "print(\"Tokenizador cargado exitosamente...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16efcf36",
   "metadata": {},
   "source": [
    "### Tokenización de archivos: spanish.txt y aymara.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19199ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_spanish = \"data/clean/spanish.txt\"\n",
    "input_aymara = \"data/clean/aymara.txt\"\n",
    "\n",
    "folder = \"data/tokenized\"\n",
    "os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "output_spanish = f\"{folder}/spanish.ids\"\n",
    "output_aymara = f\"{folder}/aymara.ids\"\n",
    "\n",
    "# Tokenización de spanish.txt\n",
    "with open(input_spanish, \"r\", encoding=\"utf-8\") as f_input, open(output_spanish, \"w\", encoding=\"utf-8\") as f_output:\n",
    "    for line in f_input:\n",
    "        text = line.strip()\n",
    "        ids = [sp.bos_id()] + sp.encode(text, out_type=int) + [sp.eos_id()] # Tokenización usando SentencePiece\n",
    "        f_output.write(\" \".join(map(str, ids)) + \"\\n\") # Guardamos la línea tokenizada\n",
    "\n",
    "# Tokenización de aymara.txt\n",
    "with open(input_aymara, \"r\", encoding=\"utf-8\") as f_input, open(output_aymara, \"w\", encoding=\"utf-8\") as f_output:\n",
    "    for line in f_input:\n",
    "        text = line.strip()\n",
    "        ids = [sp.bos_id()] + sp.encode(text, out_type=int) + [sp.eos_id()] # Tokenización usando SentencePiece\n",
    "        f_output.write(\" \".join(map(str, ids)) + \"\\n\") # Guardamos la línea tokenizada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1e1456",
   "metadata": {},
   "source": [
    "## **Análisis de las longitudes de las secuencias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c7cb919",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanish_tokenized = \"data/tokenized/spanish.ids\"\n",
    "aymara_tokenized = \"data/tokenized/aymara.ids\"\n",
    "\n",
    "pair_lengths = []  # aquí guardamos la cantidad máxima de tokens de cada par\n",
    "with open(spanish_tokenized, \"r\", encoding=\"utf-8\") as f_spanish, open(aymara_tokenized, \"r\", encoding=\"utf-8\") as f_aymara:\n",
    "    for spanish_line, aymara_line in zip(f_spanish, f_aymara):\n",
    "        spanish_string = spanish_line.strip()\n",
    "        spanish_ids = spanish_string.split()\n",
    "        spanish_len = len(spanish_ids)\n",
    "\n",
    "        aymara_string = aymara_line.strip()\n",
    "        aymara_ids = aymara_string.split()\n",
    "        aymara_len = len(aymara_ids)\n",
    "\n",
    "        max_pair_len = max(spanish_len, aymara_len)\n",
    "        pair_lengths.append(max_pair_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa74248d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentile-90%: 51 tokens\n",
      "percentile-95%: 58 tokens\n",
      "percentile-98%: 66 tokens\n",
      "percentile-99%: 72 tokens\n",
      "percentile-100%: 153 tokens\n"
     ]
    }
   ],
   "source": [
    "pair_lengths = np.array(pair_lengths)\n",
    "\n",
    "for perc in [90, 95, 98, 99, 100]:\n",
    "    print(f\"percentile-{perc}%: {np.percentile(pair_lengths, perc):.0f} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca182f9",
   "metadata": {},
   "source": [
    "### Resultados de las longitudes\n",
    "* percentile-90%: 51 tokens\n",
    "* percentile-95%: 58 tokens\n",
    "* percentile-98%: 66 tokens\n",
    "* percentile-99%: 72 tokens\n",
    "* percentile-100%: 153 tokens\n",
    "\n",
    "Establecemos una longitud máxima de 80 tokens, ya que este valor cubre más del 99 % de los pares del corpus y es computacionalmente eficiente para el entrenamiento del Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf1e67",
   "metadata": {},
   "source": [
    "## **Creamos los splits: train / valid / test**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef0e8e9",
   "metadata": {},
   "source": [
    "### Librerías a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19d11c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdf16ab",
   "metadata": {},
   "source": [
    "### Cálculo de los splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9268e82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pares tras filtrado por longitud máxima de tokens --> 163733\n"
     ]
    }
   ],
   "source": [
    "SEED = 36\n",
    "random.seed(SEED)\n",
    "\n",
    "MAX_LEN = 80 # Máxima longitud de los pares (español, aymara) --> cubre más del 99% de los pares del corpus\n",
    "\n",
    "spanish_tokenized = \"data/tokenized/spanish.ids\"\n",
    "aymara_tokenized  = \"data/tokenized/aymara.ids\"\n",
    "spanish_clean     = \"data/clean/spanish.txt\"\n",
    "aymara_clean      = \"data/clean/aymara.txt\"\n",
    "\n",
    "pairs = []  # (es_ids, ay_ids, es_txt, ay_txt)\n",
    "\n",
    "with open(spanish_tokenized, encoding=\"utf-8\") as f_es_ids, \\\n",
    "     open(aymara_tokenized, encoding=\"utf-8\") as f_ay_ids, \\\n",
    "     open(spanish_clean, encoding=\"utf-8\") as f_es_txt, \\\n",
    "     open(aymara_clean, encoding=\"utf-8\") as f_ay_txt:\n",
    "\n",
    "    for es_ids, ay_ids, es_txt, ay_txt in zip(f_es_ids, f_ay_ids, f_es_txt, f_ay_txt):\n",
    "        es_ids = es_ids.strip()\n",
    "        ay_ids = ay_ids.strip()\n",
    "        es_txt = es_txt.strip()\n",
    "        ay_txt = ay_txt.strip()\n",
    "\n",
    "        if not es_ids or not ay_ids:\n",
    "            continue\n",
    "\n",
    "        if len(es_ids.split()) <= MAX_LEN and len(ay_ids.split()) <= MAX_LEN:\n",
    "            pairs.append((es_ids, ay_ids, es_txt, ay_txt))\n",
    "\n",
    "print(f\"Pares tras filtrado por longitud máxima de tokens --> {len(pairs)}\")\n",
    "\n",
    "random.shuffle(pairs)\n",
    "\n",
    "n = len(pairs)\n",
    "train_end = int(0.90 * n)\n",
    "valid_end = int(0.95 * n)\n",
    "\n",
    "train = pairs[:train_end]\n",
    "valid = pairs[train_end:valid_end]\n",
    "test  = pairs[valid_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbd07a1",
   "metadata": {},
   "source": [
    "### Dataset para Zenodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_zenodo = \"data/zenodo\"\n",
    "os.makedirs(output_folder_zenodo, exist_ok=True) # creamos carpeta si no existe\n",
    "\n",
    "with open(f\"{output_folder_zenodo}/spanish.txt\", \"w\", encoding=\"utf-8\") as f_es_txt, \\\n",
    "     open(f\"{output_folder_zenodo}/aymara.txt\",  \"w\", encoding=\"utf-8\") as f_ay_txt:\n",
    "\n",
    "    for _, _, es_txt, ay_txt in pairs:\n",
    "        f_es_txt.write(es_txt + \"\\n\")\n",
    "        f_ay_txt.write(ay_txt + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b5e44",
   "metadata": {},
   "source": [
    "### Guardamos los splits: train / val / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "de67af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder_ids = \"data/splits/ids\"\n",
    "os.makedirs(output_folder_ids, exist_ok=True) # creamos carpeta si no existe\n",
    "output_folder_texts = \"data/splits/texts\"\n",
    "os.makedirs(output_folder_texts, exist_ok=True) # creamos carpeta si no existe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0253fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training set\n",
    "with open(f\"{output_folder_ids}/train.spanish\", \"w\", encoding=\"utf-8\") as f_es_ids, \\\n",
    "     open(f\"{output_folder_ids}/train.aymara\",  \"w\", encoding=\"utf-8\") as f_ay_ids, \\\n",
    "     open(f\"{output_folder_texts}/train.spanish\", \"w\", encoding=\"utf-8\") as f_es_txt, \\\n",
    "     open(f\"{output_folder_texts}/train.aymara\",  \"w\", encoding=\"utf-8\") as f_ay_txt:\n",
    "\n",
    "    for es_ids, ay_ids, es_txt, ay_txt in train:\n",
    "        f_es_ids.write(es_ids + \"\\n\")\n",
    "        f_ay_ids.write(ay_ids + \"\\n\")\n",
    "        f_es_txt.write(es_txt + \"\\n\")\n",
    "        f_ay_txt.write(ay_txt + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "71f585b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set\n",
    "with open(f\"{output_folder_ids}/valid.spanish\", \"w\", encoding=\"utf-8\") as f_es_ids, \\\n",
    "     open(f\"{output_folder_ids}/valid.aymara\",  \"w\", encoding=\"utf-8\") as f_ay_ids, \\\n",
    "     open(f\"{output_folder_texts}/valid.spanish\", \"w\", encoding=\"utf-8\") as f_es_txt, \\\n",
    "     open(f\"{output_folder_texts}/valid.aymara\",  \"w\", encoding=\"utf-8\") as f_ay_txt:\n",
    "\n",
    "    for es_ids, ay_ids, es_txt, ay_txt in valid:\n",
    "        f_es_ids.write(es_ids + \"\\n\")\n",
    "        f_ay_ids.write(ay_ids + \"\\n\")\n",
    "        f_es_txt.write(es_txt + \"\\n\")\n",
    "        f_ay_txt.write(ay_txt + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d241da7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "with open(f\"{output_folder_ids}/test.spanish\", \"w\", encoding=\"utf-8\") as f_es_ids, \\\n",
    "     open(f\"{output_folder_ids}/test.aymara\",  \"w\", encoding=\"utf-8\") as f_ay_ids, \\\n",
    "     open(f\"{output_folder_texts}/test.spanish\", \"w\", encoding=\"utf-8\") as f_es_txt, \\\n",
    "     open(f\"{output_folder_texts}/test.aymara\",  \"w\", encoding=\"utf-8\") as f_ay_txt:\n",
    "\n",
    "    for es_ids, ay_ids, es_txt, ay_txt in test:\n",
    "        f_es_ids.write(es_ids + \"\\n\")\n",
    "        f_ay_ids.write(ay_ids + \"\\n\")\n",
    "        f_es_txt.write(es_txt + \"\\n\")\n",
    "        f_ay_txt.write(ay_txt + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d96cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bible",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
