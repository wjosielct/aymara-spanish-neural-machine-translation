{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7374ee3",
   "metadata": {},
   "source": [
    "# **Entrenamiento Español $\\rightarrow$ Aymara (Arquitectura Deep Encoder - Shallow Decoder)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7cb69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentencepiece\n",
    "!pip install -q sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60ad4ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.amp import autocast, GradScaler\n",
    "import sentencepiece as spm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "import os\n",
    "import sacrebleu\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4766668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. DATASET Y DATALOADER\n",
    "# ==========================================\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_path, trgt_path):\n",
    "        with open(src_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.src_lines = [line.strip() for line in f]\n",
    "        with open(trgt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.trgt_lines = [line.strip() for line in f]\n",
    "        assert len(self.src_lines) == len(self.trgt_lines)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = list(map(int, self.src_lines[idx].split()))\n",
    "        trgt = list(map(int, self.trgt_lines[idx].split()))\n",
    "        # Aseguramos long para embeddings\n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(trgt, dtype=torch.long)\n",
    "\n",
    "train_dataset = TranslationDataset(src_path=\"data/splits/ids/train.spanish\", trgt_path=\"data/splits/ids/train.aymara\")\n",
    "valid_dataset = TranslationDataset(src_path=\"data/splits/ids/valid.spanish\", trgt_path=\"data/splits/ids/valid.aymara\")\n",
    "test_dataset  = TranslationDataset(src_path=\"data/splits/ids/test.spanish\",  trgt_path=\"data/splits/ids/test.aymara\")\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tokenizer/SentencePiece.model\")\n",
    "PAD_ID, BOS_ID, EOS_ID = sp.pad_id(), sp.bos_id(), sp.eos_id()\n",
    "vocab_size = sp.vocab_size()\n",
    "\n",
    "def collate_fn_batch(batch):\n",
    "    src_batch, trgt_batch = zip(*batch)\n",
    "    src_padded = pad_sequence(src_batch, batch_first=True, padding_value=PAD_ID)\n",
    "    trgt_padded = pad_sequence(trgt_batch, batch_first=True, padding_value=PAD_ID)\n",
    "    return src_padded, trgt_padded\n",
    "\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_batch)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_batch)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5305004a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 2. TRANSFORMER\n",
    "# ==========================================\n",
    "\n",
    "class TranslationTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, n_heads=8, num_encoder_layers=6, num_decoder_layers=6, dim_ffnn=2048, dropout=0.1, pad_id=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.positional_encoder = nn.Embedding(150, d_model)\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_ffnn,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=d_model, out_features=vocab_size, bias=False)\n",
    "        self.output_layer.weight = self.embedding.weight \n",
    "\n",
    "\n",
    "    def forward(self, src, trgt, trgt_causal_mask=None, src_key_padding_mask=None, trgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        src_pos = self.positional_encoder(torch.arange(src.size(1), device=src.device))\n",
    "        trgt_pos = self.positional_encoder(torch.arange(trgt.size(1), device=trgt.device))\n",
    "\n",
    "        # Escalado por sqrt(d_model)\n",
    "        src = (self.embedding(src) * math.sqrt(self.d_model)) + src_pos\n",
    "        trgt = (self.embedding(trgt) * math.sqrt(self.d_model)) + trgt_pos\n",
    "\n",
    "        out = self.transformer(\n",
    "            src,\n",
    "            trgt,\n",
    "            tgt_mask=trgt_causal_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=trgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "        return self.output_layer(out)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Dispositivo: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07eef9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. INICIALIZACIÓN\n",
    "# ==========================================\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "model = TranslationTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    num_encoder_layers=12,\n",
    "    num_decoder_layers=2,\n",
    "    dim_ffnn=2048,\n",
    "    dropout=0.3,\n",
    "    pad_id=PAD_ID\n",
    ").to(device)\n",
    "\n",
    "def init_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, betas=(0.9, 0.98), eps=1e-9, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=0.1)\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.0005, total_steps=total_steps, pct_start=0.1, anneal_strategy='cos'\n",
    ")\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "700d053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. FUNCIONES DE ENTRENAMIENTO Y DECODING\n",
    "# ==========================================\n",
    "\n",
    "def create_padding_mask(batch, pad_id):\n",
    "    return (batch == pad_id)\n",
    "\n",
    "def create_causal_mask(size, device):\n",
    "    return nn.Transformer.generate_square_subsequent_mask(size).to(device)\n",
    "\n",
    "def train_epoch_amp(model, data_loader, optimizer, criterion, device, pad_id, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for src, trgt in data_loader:\n",
    "        src, trgt = src.to(device), trgt.to(device)\n",
    "        trgt_in, trgt_out = trgt[:, :-1], trgt[:, 1:].contiguous()\n",
    "\n",
    "        trgt_causal_mask = create_causal_mask(trgt_in.size(1), device)\n",
    "        src_mask = create_padding_mask(src, pad_id)\n",
    "        trgt_mask = create_padding_mask(trgt_in, pad_id)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast(device_type='cuda', dtype=torch.float16):\n",
    "            logits = model(src, trgt_in, trgt_causal_mask, src_mask, trgt_mask, src_mask)\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), trgt_out.reshape(-1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, data_loader, criterion, device, pad_id):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    for src, trgt in data_loader:\n",
    "        src, trgt = src.to(device), trgt.to(device)\n",
    "        trgt_in, trgt_out = trgt[:, :-1], trgt[:, 1:].contiguous()\n",
    "        \n",
    "        trgt_causal_mask = create_causal_mask(trgt_in.size(1), device)\n",
    "        src_mask, trgt_mask = create_padding_mask(src, pad_id), create_padding_mask(trgt_in, pad_id)\n",
    "\n",
    "        logits = model(src, trgt_in, trgt_causal_mask, src_mask, trgt_mask, src_mask)\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), trgt_out.reshape(-1))\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_search_decode(model, src, sp, beam_size, max_len, device):\n",
    "    PAD_ID, BOS_ID, EOS_ID = sp.pad_id(), sp.bos_id(), sp.eos_id()\n",
    "    src = src.unsqueeze(0).to(device)\n",
    "    src_mask = create_padding_mask(src, PAD_ID).to(device)\n",
    "    \n",
    "    # Encoding manual debe considerar el escalado sqrt(d_model)\n",
    "    src_emb = model.embedding(src) * math.sqrt(model.d_model)\n",
    "    src_pos = model.positional_encoder(torch.arange(src.size(1), device=device))\n",
    "    memory = model.transformer.encoder(src_emb + src_pos, src_key_padding_mask=src_mask)\n",
    "\n",
    "    candidates = [([BOS_ID], 0.0)]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        new_candidates = []\n",
    "        all_finished = True\n",
    "        \n",
    "        for seq, score in candidates:\n",
    "            if seq[-1] == EOS_ID:\n",
    "                new_candidates.append((seq, score))\n",
    "                continue\n",
    "            \n",
    "            all_finished = False\n",
    "            trgt_in = torch.tensor([seq], device=device)\n",
    "            \n",
    "            # Decoder forward manual también con escalado\n",
    "            trgt_emb = model.embedding(trgt_in) * math.sqrt(model.d_model)\n",
    "            trgt_pos = model.positional_encoder(torch.arange(trgt_in.size(1), device=device))\n",
    "            \n",
    "            trgt_causal_mask = create_causal_mask(trgt_in.size(1), device)\n",
    "            out = model.transformer.decoder(trgt_emb + trgt_pos, memory, tgt_mask=trgt_causal_mask, memory_key_padding_mask=src_mask)\n",
    "            \n",
    "            log_probs = torch.log_softmax(model.output_layer(out[:, -1, :]), dim=-1)\n",
    "            topk_probs, topk_ids = torch.topk(log_probs, beam_size, dim=-1)\n",
    "            \n",
    "            for i in range(beam_size):\n",
    "                new_candidates.append((seq + [topk_ids[0][i].item()], score + topk_probs[0][i].item()))\n",
    "        \n",
    "        if all_finished: break\n",
    "        candidates = sorted(new_candidates, key=lambda x: x[1] / (len(x[0]) ** 0.7), reverse=True)[:beam_size]\n",
    "        \n",
    "    return candidates[0][0]\n",
    "\n",
    "def bleu_epoch(model, dataset, sp, device, beam_size=3, max_len=80, limit=1000):\n",
    "    model.eval()\n",
    "    hypotheses, references = [], []\n",
    "    \n",
    "    total_samples = len(dataset)\n",
    "    \n",
    "    if limit is None:\n",
    "        n_eval = total_samples # Si es None, usamos todo el data set\n",
    "    else:\n",
    "        n_eval = min(total_samples, limit)\n",
    "        \n",
    "    indices = range(n_eval)\n",
    "    # ------------------\n",
    "    \n",
    "    if n_eval > 1000:\n",
    "        print(f\"Evaluando {n_eval} oraciones (esto tomará un tiempo)...\")\n",
    "\n",
    "    for idx in indices:\n",
    "        src, trgt = dataset[idx]\n",
    "        references.append(sp.decode(trgt.tolist()))\n",
    "        hypotheses.append(sp.decode(beam_search_decode(model, src, sp, beam_size, max_len, device)))\n",
    "        \n",
    "    return sacrebleu.corpus_bleu(hypotheses, [references]).score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3753cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando 100 épocas. Validando BLEU cada 10 épocas (max 1000 oraciones).\n",
      "Epoch 1 | LR: 0.00003175 | Train Loss: 7.5713 | Val Loss: 6.8050 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 2 | LR: 0.00006584 | Train Loss: 6.5440 | Val Loss: 6.1698 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 3 | LR: 0.00011895 | Train Loss: 5.9229 | Val Loss: 5.5750 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 4 | LR: 0.00018586 | Train Loss: 5.3793 | Val Loss: 5.0689 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 5 | LR: 0.00026003 | Train Loss: 4.8848 | Val Loss: 4.5864 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 6 | LR: 0.00033420 | Train Loss: 4.5044 | Val Loss: 4.2782 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 7 | LR: 0.00040111 | Train Loss: 4.2498 | Val Loss: 4.0783 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 8 | LR: 0.00045419 | Train Loss: 4.0735 | Val Loss: 3.9345 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 9 | LR: 0.00048827 | Train Loss: 3.9384 | Val Loss: 3.8126 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "--> ¡Nuevo récord BLEU: 4.39!\n",
      "Epoch 10 | LR: 0.00050000 | Train Loss: 3.8206 | Val Loss: 3.7034 | Val BLEU: 4.39\n",
      "------------------------------------------------------------\n",
      "Epoch 11 | LR: 0.00049985 | Train Loss: 3.7095 | Val Loss: 3.6079 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 12 | LR: 0.00049939 | Train Loss: 3.6102 | Val Loss: 3.5230 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 13 | LR: 0.00049863 | Train Loss: 3.5229 | Val Loss: 3.4608 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 14 | LR: 0.00049757 | Train Loss: 3.4435 | Val Loss: 3.3944 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 15 | LR: 0.00049620 | Train Loss: 3.3727 | Val Loss: 3.3434 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 16 | LR: 0.00049454 | Train Loss: 3.3069 | Val Loss: 3.2928 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 17 | LR: 0.00049257 | Train Loss: 3.2466 | Val Loss: 3.2418 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 18 | LR: 0.00049031 | Train Loss: 3.1897 | Val Loss: 3.2058 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 19 | LR: 0.00048776 | Train Loss: 3.1362 | Val Loss: 3.1565 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "--> ¡Nuevo récord BLEU: 8.83!\n",
      "Epoch 20 | LR: 0.00048492 | Train Loss: 3.0861 | Val Loss: 3.1205 | Val BLEU: 8.83\n",
      "------------------------------------------------------------\n",
      "Epoch 21 | LR: 0.00048179 | Train Loss: 3.0378 | Val Loss: 3.0934 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 22 | LR: 0.00047838 | Train Loss: 2.9911 | Val Loss: 3.0567 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 23 | LR: 0.00047470 | Train Loss: 2.9472 | Val Loss: 3.0319 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 24 | LR: 0.00047073 | Train Loss: 2.9070 | Val Loss: 3.0021 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 25 | LR: 0.00046650 | Train Loss: 2.8680 | Val Loss: 2.9717 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 26 | LR: 0.00046201 | Train Loss: 2.8298 | Val Loss: 2.9443 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 27 | LR: 0.00045726 | Train Loss: 2.7926 | Val Loss: 2.9156 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 28 | LR: 0.00045225 | Train Loss: 2.7581 | Val Loss: 2.8965 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 29 | LR: 0.00044700 | Train Loss: 2.7253 | Val Loss: 2.8768 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "--> ¡Nuevo récord BLEU: 11.13!\n",
      "Epoch 30 | LR: 0.00044151 | Train Loss: 2.6932 | Val Loss: 2.8577 | Val BLEU: 11.13\n",
      "------------------------------------------------------------\n",
      "Epoch 31 | LR: 0.00043578 | Train Loss: 2.6620 | Val Loss: 2.8376 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 32 | LR: 0.00042983 | Train Loss: 2.6332 | Val Loss: 2.8141 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 33 | LR: 0.00042366 | Train Loss: 2.6045 | Val Loss: 2.7965 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 34 | LR: 0.00041728 | Train Loss: 2.5763 | Val Loss: 2.7784 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 35 | LR: 0.00041069 | Train Loss: 2.5494 | Val Loss: 2.7580 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 36 | LR: 0.00040391 | Train Loss: 2.5241 | Val Loss: 2.7443 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 37 | LR: 0.00039694 | Train Loss: 2.5005 | Val Loss: 2.7268 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 38 | LR: 0.00038979 | Train Loss: 2.4759 | Val Loss: 2.7106 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 39 | LR: 0.00038247 | Train Loss: 2.4533 | Val Loss: 2.6938 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "--> ¡Nuevo récord BLEU: 12.87!\n",
      "Epoch 40 | LR: 0.00037499 | Train Loss: 2.4305 | Val Loss: 2.6790 | Val BLEU: 12.87\n",
      "------------------------------------------------------------\n",
      "Epoch 41 | LR: 0.00036736 | Train Loss: 2.4096 | Val Loss: 2.6606 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 42 | LR: 0.00035959 | Train Loss: 2.3895 | Val Loss: 2.6510 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 43 | LR: 0.00035168 | Train Loss: 2.3687 | Val Loss: 2.6375 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 44 | LR: 0.00034365 | Train Loss: 2.3492 | Val Loss: 2.6285 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 45 | LR: 0.00033550 | Train Loss: 2.3304 | Val Loss: 2.6139 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 46 | LR: 0.00032725 | Train Loss: 2.3113 | Val Loss: 2.6061 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 47 | LR: 0.00031890 | Train Loss: 2.2936 | Val Loss: 2.5907 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 48 | LR: 0.00031047 | Train Loss: 2.2762 | Val Loss: 2.5772 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 49 | LR: 0.00030197 | Train Loss: 2.2600 | Val Loss: 2.5655 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 50 | LR: 0.00029341 | Train Loss: 2.2435 | Val Loss: 2.5618 | Val BLEU: 11.42\n",
      "------------------------------------------------------------\n",
      "Epoch 51 | LR: 0.00028479 | Train Loss: 2.2275 | Val Loss: 2.5461 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 52 | LR: 0.00027613 | Train Loss: 2.2117 | Val Loss: 2.5335 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 53 | LR: 0.00026743 | Train Loss: 2.1967 | Val Loss: 2.5253 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 54 | LR: 0.00025872 | Train Loss: 2.1825 | Val Loss: 2.5183 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 55 | LR: 0.00024999 | Train Loss: 2.1683 | Val Loss: 2.5092 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 56 | LR: 0.00024127 | Train Loss: 2.1547 | Val Loss: 2.4990 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 57 | LR: 0.00023255 | Train Loss: 2.1412 | Val Loss: 2.4902 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 58 | LR: 0.00022386 | Train Loss: 2.1273 | Val Loss: 2.4790 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 59 | LR: 0.00021520 | Train Loss: 2.1150 | Val Loss: 2.4737 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 60 | LR: 0.00020658 | Train Loss: 2.1029 | Val Loss: 2.4637 | Val BLEU: 12.35\n",
      "------------------------------------------------------------\n",
      "Epoch 61 | LR: 0.00019802 | Train Loss: 2.0901 | Val Loss: 2.4625 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 62 | LR: 0.00018951 | Train Loss: 2.0784 | Val Loss: 2.4507 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 63 | LR: 0.00018108 | Train Loss: 2.0679 | Val Loss: 2.4452 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 64 | LR: 0.00017274 | Train Loss: 2.0561 | Val Loss: 2.4354 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 65 | LR: 0.00016449 | Train Loss: 2.0447 | Val Loss: 2.4302 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 66 | LR: 0.00015634 | Train Loss: 2.0348 | Val Loss: 2.4268 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 67 | LR: 0.00014831 | Train Loss: 2.0242 | Val Loss: 2.4195 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 68 | LR: 0.00014040 | Train Loss: 2.0149 | Val Loss: 2.4139 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 69 | LR: 0.00013263 | Train Loss: 2.0050 | Val Loss: 2.4072 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 70 | LR: 0.00012499 | Train Loss: 1.9965 | Val Loss: 2.4049 | Val BLEU: 11.03\n",
      "------------------------------------------------------------\n",
      "Epoch 71 | LR: 0.00011752 | Train Loss: 1.9875 | Val Loss: 2.3968 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 72 | LR: 0.00011020 | Train Loss: 1.9784 | Val Loss: 2.3909 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 73 | LR: 0.00010305 | Train Loss: 1.9710 | Val Loss: 2.3846 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 74 | LR: 0.00009608 | Train Loss: 1.9622 | Val Loss: 2.3823 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 75 | LR: 0.00008930 | Train Loss: 1.9546 | Val Loss: 2.3789 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 76 | LR: 0.00008271 | Train Loss: 1.9479 | Val Loss: 2.3746 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 77 | LR: 0.00007633 | Train Loss: 1.9402 | Val Loss: 2.3704 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 78 | LR: 0.00007016 | Train Loss: 1.9336 | Val Loss: 2.3655 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 79 | LR: 0.00006421 | Train Loss: 1.9274 | Val Loss: 2.3631 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 80 | LR: 0.00005849 | Train Loss: 1.9216 | Val Loss: 2.3598 | Val BLEU: 10.89\n",
      "------------------------------------------------------------\n",
      "Epoch 81 | LR: 0.00005299 | Train Loss: 1.9159 | Val Loss: 2.3552 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 82 | LR: 0.00004774 | Train Loss: 1.9109 | Val Loss: 2.3516 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 83 | LR: 0.00004274 | Train Loss: 1.9059 | Val Loss: 2.3497 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 84 | LR: 0.00003799 | Train Loss: 1.9014 | Val Loss: 2.3473 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 85 | LR: 0.00003349 | Train Loss: 1.8969 | Val Loss: 2.3453 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 86 | LR: 0.00002926 | Train Loss: 1.8926 | Val Loss: 2.3431 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 87 | LR: 0.00002530 | Train Loss: 1.8893 | Val Loss: 2.3417 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 88 | LR: 0.00002161 | Train Loss: 1.8858 | Val Loss: 2.3400 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 89 | LR: 0.00001820 | Train Loss: 1.8827 | Val Loss: 2.3385 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 90 | LR: 0.00001508 | Train Loss: 1.8803 | Val Loss: 2.3373 | Val BLEU: 11.14\n",
      "------------------------------------------------------------\n",
      "Epoch 91 | LR: 0.00001224 | Train Loss: 1.8778 | Val Loss: 2.3360 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 92 | LR: 0.00000968 | Train Loss: 1.8753 | Val Loss: 2.3348 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 93 | LR: 0.00000743 | Train Loss: 1.8738 | Val Loss: 2.3339 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 94 | LR: 0.00000546 | Train Loss: 1.8726 | Val Loss: 2.3336 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 95 | LR: 0.00000380 | Train Loss: 1.8711 | Val Loss: 2.3320 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 96 | LR: 0.00000243 | Train Loss: 1.8700 | Val Loss: 2.3324 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 97 | LR: 0.00000137 | Train Loss: 1.8694 | Val Loss: 2.3322 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 98 | LR: 0.00000061 | Train Loss: 1.8693 | Val Loss: 2.3325 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "Epoch 99 | LR: 0.00000015 | Train Loss: 1.8690 | Val Loss: 2.3325 | Val BLEU: -\n",
      "------------------------------------------------------------\n",
      "--> Calculando BLEU...\n",
      "Epoch 100 | LR: 0.00000000 | Train Loss: 1.8682 | Val Loss: 2.3325 | Val BLEU: 10.73\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. LOOP PRINCIPAL\n",
    "# ==========================================\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_bleu = 0.0\n",
    "training_history = []\n",
    "\n",
    "print(f\"Entrenando {EPOCHS} épocas. Validando BLEU cada 10 épocas (max 1000 oraciones).\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss = train_epoch_amp(model, train_loader, optimizer, criterion, device, PAD_ID, scheduler)\n",
    "    val_loss = eval_epoch(model, valid_loader, criterion, device, PAD_ID)\n",
    "    val_ppl = math.exp(min(val_loss, 100))\n",
    "    current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"models/best_loss_model_SpanishToAymara_DeepShallow.pt\")\n",
    "\n",
    "    bleu_score = None\n",
    "    if epoch % 10 == 0 or epoch == EPOCHS:\n",
    "        print(\"--> Calculando BLEU...\")\n",
    "        # Usamos beam_size=3 para calidad\n",
    "        bleu_score = bleu_epoch(model, valid_dataset, sp, device, beam_size=3, max_len=80, limit=1000)\n",
    "        \n",
    "        if bleu_score > best_bleu:\n",
    "            best_bleu = bleu_score\n",
    "            torch.save(model.state_dict(), \"models/best_bleu_model_SpanishToAymara_DeepShallow.pt\")\n",
    "            print(f\"--> ¡Nuevo récord BLEU: {best_bleu:.2f}!\")\n",
    "\n",
    "    bleu_str = f\"{bleu_score:.2f}\" if bleu_score is not None else \"-\"\n",
    "    print(f\"Epoch {epoch} | LR: {current_lr:.8f} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val BLEU: {bleu_str}\")\n",
    "\n",
    "    training_history.append({\n",
    "        \"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss,\n",
    "        \"val_ppl\": val_ppl, \"val_bleu\": bleu_score, \"lr\": current_lr\n",
    "    })\n",
    "    pd.DataFrame(training_history).to_csv(\"results/metrics_SpanishToAymara_DeepShallow.csv\", index=False)\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d837a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando el mejor modelo para evaluación del conjunto de prueba...\n",
      "--> Cargando el mejor modelo guardado: models/best_bleu_model_SpanishToAymara_DeepEncoder-ShallowDecoder.pt\n",
      "Pesos del modelo cargados exitosamente!\n",
      "\n",
      "Calculando métricas en el Test Set completo...\n",
      "Evaluando 8187 oraciones (esto tomará un tiempo)...\n",
      "\n",
      "Resultados Finales del Test Set:\n",
      "===================================\n",
      "Test Loss: 2.6883\n",
      "Test PPL : 14.71\n",
      "Test BLEU: 11.81 (Beam=5)\n",
      "===================================\n",
      "\n",
      "--- Ejemplos de Traducción ---\n",
      "Español:  molerás una parte de él muy fina y la pondrás delante del testimonio, en el tabernáculo de reunión, donde yo me encontraré contigo. será para ustedes cosa muy sagrada.\n",
      "Aymara: mä chicatsti suma ñut'u q'iyjasinjja arca nayrakataruw uchäta tatitumpi jiquisiñ carpa mankhana, cawqhantejj nayajj jumampi jiquiscañäni uca chekaru; aca inciensosti wali kollanäniwa jumanacataquejja.\n",
      "Modelo:  mä chikatxa suma ñut'u k'iyjasin arca nayraqataruw uchäta tatitump jikisiñ carpa manqharu, kawkhantix nayax jumamp jikiskä uka chiqaru. uka luqtäwix jumanakatak wali qullanapunïniwa.\n",
      "--------------------------------------------------\n",
      "Español:  de día los condujo con una nube; toda la noche con resplandor de fuego.\n",
      "Aymara: mä kenayaw urojj irpäna, nina khanaw arumajj irparaquïna,\n",
      "Modelo:  mä qinayaw irpäna, arum ch'amakarusti nina irparakïna.\n",
      "--------------------------------------------------\n",
      "Español:  mientras ellos comían, jesús tomó pan y lo bendijo; lo partió y lo dio a sus discípulos, y dijo: -tomen; coman. esto es mi cuerpo.\n",
      "Aymara: manq'apkipansti, jesusax t'ant'a irtasin yuspagaräna, pachjasinsti yatiqirinakaparuw churäna: -manq'apxam; akax nayan janchixawa -sasa.\n",
      "Modelo:  mank'ascasinsti, jesusajj t'ant'a irtasinwa diosar yuspagaräna. pachjasinsti discipulonacaparuw churäna: -mank'apjjam, acajj janchejjawa -sasa.\n",
      "--------------------------------------------------\n",
      "Español:  jesús le dijo entonces: -vuelve a casa; tu hijo vive. el oficial creyó lo que jesús le dijo, y se fue.\n",
      "Aymara: ukat jesusax jupar säna: -utamar kuttxam, wawamax jakaskiwa -sasa. jilïr jaqisti, kuntix jesusax siskän uk iyawsänwa, ukat sarawayxäna.\n",
      "Modelo:  jesusasti juparux sänwa: -utamar kuttxam, wawamax jakaskiwa -sasa. wawamax iyawsänwa, ukat sarawayxäna.\n",
      "--------------------------------------------------\n",
      "Español:  hizo las granadas en dos hileras alrededor de cada red, para cubrir los capiteles de la parte superior de las columnas .\n",
      "Aymara: luraraquïnwa pä seke granadanaca sapakata rejillanaca nayrakataru muytata, sayirinaca patjjancqui uca capitelenacaru imantañataqui.\n",
      "Modelo:  ucjjarusti luraraquïnwa pä seke pä seke granadanaca sapakata, sayirinaca patjjancqui uca capitelenacaru imantañataqui.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. EVALUACIÓN FINAL EN TEST SET\n",
    "# ==========================================\n",
    "\n",
    "print(\"Cargando el mejor modelo para evaluación del conjunto de prueba...\")\n",
    "\n",
    "# 1. Re-inicializamos el modelo limpio\n",
    "model_test = TranslationTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    num_encoder_layers=12,\n",
    "    num_decoder_layers=2,\n",
    "    dim_ffnn=2048,\n",
    "    dropout=0.0, # Dropout no se usa en inferencia\n",
    "    pad_id=PAD_ID\n",
    ").to(device)\n",
    "\n",
    "# 2. Cargas los pesos. \n",
    "model_path = \"models/best_bleu_model_SpanishToAymara_DeepShallow.pt\"\n",
    "print(f\"--> Cargando el mejor modelo guardado: {model_path}\")\n",
    "model_test.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(\"Pesos del modelo cargados exitosamente!\")\n",
    "\n",
    "model_test.eval()\n",
    "\n",
    "# ------------------------------------------\n",
    "# A. Cálculo de Métricas Cuantitativas\n",
    "# ------------------------------------------\n",
    "print(\"\\nCalculando métricas en el Test Set completo...\")\n",
    "\n",
    "# 1. Loss y Perplexity\n",
    "test_loss = eval_epoch(model_test, test_loader, criterion, device, PAD_ID)\n",
    "test_ppl = math.exp(min(test_loss, 100))\n",
    "\n",
    "# 2. BLEU Score\n",
    "# Nota: Ponemos limit=None para evaluar el test set completo\n",
    "# Nota: Beam size=5 es estándar para publicaciones y resultados finales\n",
    "test_bleu = bleu_epoch(model_test, test_dataset, sp, device, beam_size=5, max_len=80, limit=None)\n",
    "\n",
    "print(f\"\\nResultados Finales del Test Set:\")\n",
    "print(f\"===================================\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test PPL : {test_ppl:.2f}\")\n",
    "print(f\"Test BLEU: {test_bleu:.2f} (Beam=5)\")\n",
    "print(f\"===================================\")\n",
    "\n",
    "# ------------------------------------------\n",
    "# B. Evaluación Cualitativa (Ver traducciones)\n",
    "# ------------------------------------------\n",
    "print(\"\\n--- Ejemplos de Traducción ---\")\n",
    "indices = random.sample(range(len(test_dataset)), 5) # 5 ejemplos al azar\n",
    "\n",
    "for idx in indices:\n",
    "    src, trgt = test_dataset[idx]\n",
    "    \n",
    "    src_text = sp.decode(src.tolist())\n",
    "    trgt_text = sp.decode(trgt.tolist())\n",
    "    \n",
    "    # Inferencia\n",
    "    pred_ids = beam_search_decode(model_test, src, sp, beam_size=5, max_len=80, device=device)\n",
    "    pred_text = sp.decode(pred_ids)\n",
    "    \n",
    "    print(f\"Español:  {src_text}\")\n",
    "    print(f\"Aymara: {trgt_text}\")\n",
    "    print(f\"Modelo:  {pred_text}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fc9723-71f8-4ca6-82f2-78c098a999c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
